<div id="toc" class="toc">
<div id="toctitle">Table of Contents</div>
<ul class="sectlevel1">
<li><a href="#key-features_str">1. Key features</a>
<ul class="sectlevel2">
<li><a href="#key-features-kafka_str">1.1. Kafka capabilities</a></li>
<li><a href="#kafka_use_cases">1.2. Kafka use cases</a></li>
<li><a href="#key-features-product_str">1.3. How Strimzi supports Kafka</a></li>
</ul>
</li>
<li><a href="#kafka-concepts_str">2. About Kafka</a>
<ul class="sectlevel2">
<li><a href="#kafka-concepts-key_str">2.1. Kafka concepts</a></li>
<li><a href="#kafka-concepts-producers-consumers_str">2.2. Producers and consumers</a></li>
</ul>
</li>
<li><a href="#kafka-components_str">3. Strimzi deployment of Kafka</a>
<ul class="sectlevel2">
<li><a href="#kafka-concepts-components_str">3.1. Kafka component architecture</a></li>
<li><a href="#overview-components-kafka-bridge_str">3.2. Kafka Bridge interface</a>
<ul class="sectlevel3">
<li><a href="#http_requests">3.2.1. HTTP requests</a></li>
<li><a href="#con-overview-components-kafka-bridge-clients_str">3.2.2. Supported clients for the Kafka Bridge</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#overview-components_str">4. Strimzi Operators</a>
<ul class="sectlevel2">
<li><a href="#overview-components-cluster-operator-str">4.1. Cluster Operator</a></li>
<li><a href="#overview-concepts-topic-operator-str">4.2. Topic Operator</a></li>
<li><a href="#overview-concepts-user-operator-str">4.3. User Operator</a></li>
</ul>
</li>
<li><a href="#configuration-points_str">5. Kafka configuration</a>
<ul class="sectlevel2">
<li><a href="#configuration-points-resources_str">5.1. Custom resources</a></li>
<li><a href="#configuration-points-common_str">5.2. Common configuration</a></li>
<li><a href="#configuration-points-broker_str">5.3. Kafka cluster configuration</a></li>
<li><a href="#configuration-points-topic_str">5.4. Kafka MirrorMaker configuration</a></li>
<li><a href="#configuration-points-connect_str">5.5. Kafka Connect configuration</a></li>
<li><a href="#configuration-points-bridge_str">5.6. Kafka Bridge configuration</a></li>
</ul>
</li>
<li><a href="#security-overview_str">6. Securing Kafka</a>
<ul class="sectlevel2">
<li><a href="#security-configuration-encryption_str">6.1. Encryption</a></li>
<li><a href="#security-configuration-authentication_str">6.2. Authentication</a></li>
<li><a href="#security-configuration-authorization_str">6.3. Authorization</a></li>
</ul>
</li>
<li><a href="#metrics-overview_str">7. Monitoring</a>
<ul class="sectlevel2">
<li><a href="#metrics-overview-tools-str">7.1. Prometheus</a></li>
<li><a href="#metrics-overview-grafana_str">7.2. Grafana</a></li>
<li><a href="#metrics-overview-exporter_str">7.3. Kafka Exporter</a></li>
<li><a href="#metrics-overview-tracing_str">7.4. Distributed tracing</a></li>
<li><a href="#metrics-overview-cruisecontrol_str">7.5. Cruise Control</a></li>
</ul>
</li>
</ul>
</div>
<div class="sect1">
<h2 id="key-features_str"><a class="link" href="#key-features_str">1. Key features</a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>Strimzi simplifies the process of running Apache Kafka in a Kubernetes cluster.</p>
</div>
<div class="paragraph">
<p>This guide is intended as a starting point for building an understanding of Strimzi.
The guide introduces some of the key concepts behind Kafka, which is central to Strimzi, explaining briefly the purpose of Kafka components.
Configuration points are outlined, including options to secure and monitor Kafka.
A distribution of Strimzi provides the files to deploy and manage a Kafka cluster, as well as example files for configuration and monitoring of your deployment.</p>
</div>
<div class="paragraph">
<p>A typical Kafka deployment is described, as well as the tools used to deploy and manage Kafka.</p>
</div>
<div class="sect2">
<h3 id="key-features-kafka_str"><a class="link" href="#key-features-kafka_str">1.1. Kafka capabilities</a></h3>
<div class="paragraph">
<p>The underlying data stream-processing capabilities and component architecture of Kafka can deliver:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Microservices and other applications to share data with extremely high throughput and low latency</p>
</li>
<li>
<p>Message ordering guarantees</p>
</li>
<li>
<p>Message rewind/replay from data storage to reconstruct an application state</p>
</li>
<li>
<p>Message compaction to remove old records when using a key-value log</p>
</li>
<li>
<p>Horizontal scalability in a cluster configuration</p>
</li>
<li>
<p>Replication of data to control fault tolerance</p>
</li>
<li>
<p>Retention of high volumes of data for immediate access</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="kafka_use_cases"><a class="link" href="#kafka_use_cases">1.2. Kafka use cases</a></h3>
<div class="paragraph">
<p>Kafka&#8217;s capabilities make it suitable for:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Event-driven architectures</p>
</li>
<li>
<p>Event sourcing to capture changes to the state of an application as a log of events</p>
</li>
<li>
<p>Message brokering</p>
</li>
<li>
<p>Website activity tracking</p>
</li>
<li>
<p>Operational monitoring through metrics</p>
</li>
<li>
<p>Log collection and aggregation</p>
</li>
<li>
<p>Commit logs for distributed systems</p>
</li>
<li>
<p>Stream processing so that applications can respond to data in real time</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="key-features-product_str"><a class="link" href="#key-features-product_str">1.3. How Strimzi supports Kafka</a></h3>
<div class="paragraph">
<p>Strimzi provides container images and Operators for running Kafka on Kubernetes.
Strimzi Operators are fundamental to the running of Strimzi.
The Operators provided with Strimzi are purpose-built with specialist operational knowledge to effectively manage Kafka.</p>
</div>
<div class="paragraph">
<p>Operators simplify the process of:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Deploying and running Kafka clusters</p>
</li>
<li>
<p>Deploying and running Kafka components</p>
</li>
<li>
<p>Configuring access to Kafka</p>
</li>
<li>
<p>Securing access to Kafka</p>
</li>
<li>
<p>Upgrading Kafka</p>
</li>
<li>
<p>Managing brokers</p>
</li>
<li>
<p>Creating and managing topics</p>
</li>
<li>
<p>Creating and managing users</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="kafka-concepts_str"><a class="link" href="#kafka-concepts_str">2. About Kafka</a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>Apache Kafka is an open-source distributed publish-subscribe messaging system for fault-tolerant real-time data feeds.</p>
</div>
<div class="ulist">
<div class="title">Additional resources</div>
<ul>
<li>
<p>For more information about Apache Kafka, see the <a href="http://kafka.apache.org" target="_blank" rel="noopener">Apache Kafka website</a>.</p>
</li>
</ul>
</div>
<div class="sect2">
<h3 id="kafka-concepts-key_str"><a class="link" href="#kafka-concepts-key_str">2.1. Kafka concepts</a></h3>
<div class="paragraph">
<p>Knowledge of the key concepts of Kafka is important in understanding how Strimzi works.</p>
</div>
<div class="paragraph">
<p>A Kafka cluster comprises multiple brokers.
Topics are used to receive and store data in a Kafka cluster.
Topics are split by partitions, where the data is written.
Partitions are replicated across topics for fault tolerance.</p>
</div>
<div class="paragraph">
<div class="title">Kafka brokers and topics</div>
<p><span class="image"><img src="images/overview/kafka-concepts-key-concepts.png" alt="Kafka brokers and topics inside a Kafka cluster" width="showing the partition leader of each topic"></span></p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Broker</dt>
<dd>
<p>A broker, sometimes referred to as a server or node, orchestrates the storage and passing of messages.</p>
</dd>
<dt class="hdlist1">Topic</dt>
<dd>
<p>A topic provides a destination for the storage of data.
Each topic is split into one or more partitions.</p>
</dd>
<dt class="hdlist1">Cluster</dt>
<dd>
<p>A group of broker instances.</p>
</dd>
<dt class="hdlist1">Partition</dt>
<dd>
<p>The number of topic partitions is defined by a topic <em>partition count</em>.</p>
</dd>
<dt class="hdlist1">Partition leader</dt>
<dd>
<p>A partition leader handles all producer requests for a topic.</p>
</dd>
<dt class="hdlist1">Partition follower</dt>
<dd>
<p>A partition follower replicates the partition data of a partition leader, optionally handling consumer requests.</p>
<div class="paragraph">
<p>Topics use a <em>replication factor</em> to configure the number of replicas of each partition within the cluster.
A topic comprises at least one partition.</p>
</div>
<div class="paragraph">
<p>An <em>in-sync</em> replica has the same number of messages as the leader.
Configuration defines how many replicas must be <em>in-sync</em> to be able to produce messages, ensuring that a message is committed only after it has been successfully copied to the replica partition.
In this way, if the leader fails the message is not lost.</p>
</div>
<div class="paragraph">
<p>In the <em>Kafka brokers and topics</em> diagram, we can see each numbered partition has a leader and two followers in replicated topics.</p>
</div>
</dd>
</dl>
</div>
</div>
<div class="sect2">
<h3 id="kafka-concepts-producers-consumers_str"><a class="link" href="#kafka-concepts-producers-consumers_str">2.2. Producers and consumers</a></h3>
<div class="paragraph">
<p>Producers and consumers send and receive messages (publish and subscribe) through brokers.
Messages comprise an optional <em>key</em> and a <em>value</em> that contains the message data, plus headers and related metadata.
The key is used to identify the subject of the message, or a property of the message.
Messages are delivered in batches, and batches and records contain headers and metadata that provide details that are useful for filtering and routing by clients, such as the timestamp and offset position for the record.</p>
</div>
<div class="paragraph">
<div class="title">Producers and consumers</div>
<p><span class="image"><img src="images/overview/kafka-concepts-producer-consumer.png" alt="A producer sends messages through a broker to a topic containing three partitions. Three consumers in a consumer group read the messages from the partitions"></span></p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Producer</dt>
<dd>
<p>A producer sends messages to a broker topic to be written to the end offset of a partition.
Messages are written to partitions by a producer on a round robin basis, or to a specific partition based on the message key.</p>
</dd>
<dt class="hdlist1">Consumer</dt>
<dd>
<p>A consumer subscribes to a topic and reads messages according to topic, partition and offset.</p>
</dd>
<dt class="hdlist1">Consumer group</dt>
<dd>
<p>Consumer groups are used to share a typically large data stream generated by multiple producers from a given topic.
Consumers are grouped using a <code>group.id</code>, allowing messages to be spread across the members.
Consumers within a group do not read data from the same partition, but can receive data from one or more partitions.</p>
</dd>
<dt class="hdlist1">Offsets</dt>
<dd>
<p>Offsets describe the position of messages within a partition.
Each message in a given partition has a unique offset, which helps identify the position of a consumer within the partition to track the number of records that have been consumed.</p>
<div class="paragraph">
<p>Committed offsets are written to an offset commit log.
A <code>__consumer_offsets</code> topic stores information on committed offsets, the position of last and next offset, according to consumer group.</p>
</div>
</dd>
</dl>
</div>
<div class="paragraph">
<div class="title">Producing and consuming data</div>
<p><span class="image"><img src="images/overview/kafka-concepts-partitions.png" alt="A producer sends a message to a broker topic; the message is written to the end offset (7). A consumer reads messages from offset 5"></span></p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="kafka-components_str"><a class="link" href="#kafka-components_str">3. Strimzi deployment of Kafka</a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>Apache Kafka components are provided for deployment to Kubernetes with the Strimzi distribution.
The Kafka components are generally run as clusters for availability.</p>
</div>
<div class="paragraph">
<p>A typical deployment incorporating Kafka components might include:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Kafka</strong> cluster of broker nodes</p>
</li>
<li>
<p><strong>ZooKeeper</strong> cluster of replicated ZooKeeper instances</p>
</li>
<li>
<p><strong>Kafka Connect</strong> cluster for external data connections</p>
</li>
<li>
<p><strong>Kafka MirrorMaker</strong> cluster to mirror the Kafka cluster in a secondary cluster</p>
</li>
<li>
<p><strong>Kafka Exporter</strong> to extract additional Kafka metrics data for monitoring</p>
</li>
<li>
<p><strong>Kafka Bridge</strong> to make HTTP-based requests to the Kafka cluster</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Not all of these components are mandatory, though you need Kafka and ZooKeeper as a minimum.
Some components can be deployed without Kafka, such as MirrorMaker or Kafka Connect.</p>
</div>
<div class="sect2">
<h3 id="kafka-concepts-components_str"><a class="link" href="#kafka-concepts-components_str">3.1. Kafka component architecture</a></h3>
<div class="paragraph">
<p>A cluster of Kafka brokers is main part of the Apache Kafka project responsible for delivering messages.</p>
</div>
<div class="paragraph">
<p>A broker uses Apache ZooKeeper for storing configuration data and for cluster coordination.
Before running Apache Kafka, an Apache ZooKeeper cluster has to be ready.</p>
</div>
<div class="paragraph">
<p>Each of the other Kafka components interact with the Kafka cluster to perform specific roles.</p>
</div>
<div class="paragraph">
<div class="title">Kafka component interaction</div>
<p><span class="image"><img src="images/overview/kafka-concepts-supporting-components.png" alt="Data flows between several Kafka components and the Kafka cluster. See the component descriptions after this image."></span></p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Apache ZooKeeper</dt>
<dd>
<p>Apache ZooKeeper is a core dependency for Kafka as it provides a cluster coordination service, storing and tracking the status of brokers and consumers. ZooKeeper is also used for leader election of partitions.</p>
</dd>
<dt class="hdlist1">Kafka Connect</dt>
<dd>
<p>Kafka Connect is an integration toolkit for streaming data between Kafka brokers and other systems using <em>Connector</em> plugins.
Kafka Connect provides a framework for integrating Kafka with an external data source or target, such as a database, for import or export of data using connectors.
Connectors are plugins that provide the connection configuration needed.</p>
<div class="ulist">
<ul>
<li>
<p>A <em>source</em> connector pushes external data into Kafka.</p>
</li>
<li>
<p>A <em>sink</em> connector extracts data  out of Kafka</p>
<div class="paragraph">
<p>External data is translated and transformed into the appropriate format.</p>
</div>
<div class="paragraph">
<p>You can deploy Kafka Connect with Source2Image support, which provides a convenient way to include connectors.</p>
</div>
</li>
</ul>
</div>
</dd>
<dt class="hdlist1">Kafka MirrorMaker</dt>
<dd>
<p>Kafka MirrorMaker replicates data between two Kafka clusters, within or across data centers.</p>
<div class="paragraph">
<p>MirrorMaker takes messages from a source Kafka cluster and writes them to a target Kafka cluster.</p>
</div>
</dd>
<dt class="hdlist1">Kafka Bridge</dt>
<dd>
<p>Kafka Bridge provides an API for integrating HTTP-based clients with a Kafka cluster.</p>
</dd>
<dt class="hdlist1">Kafka Exporter</dt>
<dd>
<p>Kafka Exporter extracts data for analysis as Prometheus metrics, primarily data relating to offsets, consumer groups, consumer lag and topics. Consumer lag is the delay between the last message written to a partition and the message currently being picked up from that partition by a consumer</p>
</dd>
</dl>
</div>
</div>
<div class="sect2">
<h3 id="overview-components-kafka-bridge_str"><a class="link" href="#overview-components-kafka-bridge_str">3.2. Kafka Bridge interface</a></h3>
<div class="paragraph">
<p>The Kafka Bridge provides a RESTful interface that allows HTTP-based clients to interact with a Kafka cluster. 
It offers the advantages of a web API connection to Strimzi, without the need for client applications to interpret the Kafka protocol.</p>
</div>
<div class="paragraph">
<p>The API has two main resources — <code>consumers</code> and <code>topics</code> — that are exposed and made accessible through endpoints to interact with consumers and producers in your Kafka cluster. The resources relate only to the Kafka Bridge, not the consumers and producers connected directly to Kafka.</p>
</div>
<div class="sect3">
<h4 id="http_requests"><a class="link" href="#http_requests">3.2.1. HTTP requests</a></h4>
<div class="paragraph">
<p>The Kafka Bridge supports HTTP requests to a Kafka cluster, with methods to:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Send messages to a topic.</p>
</li>
<li>
<p>Retrieve messages from topics.</p>
</li>
<li>
<p>Retrieve a list of partitions for a topic.</p>
</li>
<li>
<p>Create and delete consumers.</p>
</li>
<li>
<p>Subscribe consumers to topics, so that they start receiving messages from those topics.</p>
</li>
<li>
<p>Retrieve a list of topics that a consumer is subscribed to.</p>
</li>
<li>
<p>Unsubscribe consumers from topics.</p>
</li>
<li>
<p>Assign partitions to consumers.</p>
</li>
<li>
<p>Commit a list of consumer offsets.</p>
</li>
<li>
<p>Seek on a partition, so that a consumer starts receiving messages from the first or last offset position, or a given offset position.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The methods provide JSON responses and HTTP response code error handling.
Messages can be sent in JSON or binary formats.</p>
</div>
<div class="paragraph">
<p>Clients can produce and consume messages without the requirement to use the native Kafka protocol.</p>
</div>
<div class="ulist">
<div class="title">Additional resources</div>
<ul>
<li>
<p>To view the API documentation, including example requests and responses, see the <a href="https://strimzi.io/docs/bridge/latest/" target="_blank" rel="noopener">Kafka Bridge API reference</a>.</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="con-overview-components-kafka-bridge-clients_str"><a class="link" href="#con-overview-components-kafka-bridge-clients_str">3.2.2. Supported clients for the Kafka Bridge</a></h4>
<div class="paragraph">
<p>You can use the Kafka Bridge to integrate both <em>internal</em> and <em>external</em> HTTP client applications with your Kafka cluster.</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Internal clients</dt>
<dd>
<p>Internal clients are container-based HTTP clients running in <em>the same</em> Kubernetes cluster as the Kafka Bridge itself.
Internal clients can access the Kafka Bridge on the host and port defined in the <code>KafkaBridge</code> custom resource.</p>
</dd>
<dt class="hdlist1">External clients</dt>
<dd>
<p>External clients are HTTP clients running <em>outside</em> the Kubernetes cluster in which the Kafka Bridge is deployed and running.
External clients can access the Kafka Bridge through an OpenShift Route, a loadbalancer service, or using an Ingress.</p>
</dd>
</dl>
</div>
<div class="paragraph">
<div class="title">HTTP internal and external client integration</div>
<p><span class="image"><img src="images/kafka-bridge.png" alt="Internal and external HTTP producers and consumers exchange data with the Kafka brokers through the Kafka Bridge"></span></p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="overview-components_str"><a class="link" href="#overview-components_str">4. Strimzi Operators</a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>Strimzi supports Kafka using <em>Operators</em> to deploy and manage the components and dependencies of Kafka to Kubernetes.</p>
</div>
<div class="paragraph">
<p>Operators are a method of packaging, deploying, and managing a Kubernetes application.
Strimzi Operators extend Kubernetes functionality, automating common and complex tasks related to a Kafka deployment.
By implementing knowledge of Kafka operations in code, Kafka administration tasks are simplified and require less manual intervention.</p>
</div>
<h3 id="key-features-operators_str" class="discrete">Operators</h3>
<div class="paragraph">
<p>Strimzi provides Operators for managing a Kafka cluster running within a Kubernetes cluster.</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Cluster Operator</dt>
<dd>
<p>Deploys and manages Apache Kafka clusters, Kafka Connect, Kafka MirrorMaker, Kafka Bridge, Kafka Exporter, and the Entity Operator</p>
</dd>
<dt class="hdlist1">Entity Operator</dt>
<dd>
<p>Comprises the Topic Operator and User Operator</p>
</dd>
<dt class="hdlist1">Topic Operator</dt>
<dd>
<p>Manages Kafka topics</p>
</dd>
<dt class="hdlist1">User Operator</dt>
<dd>
<p>Manages Kafka users</p>
</dd>
</dl>
</div>
<div class="paragraph">
<p>The Cluster Operator can deploy the Topic Operator and User Operator as part of an <strong>Entity Operator</strong> configuration at the same time as a Kafka cluster.</p>
</div>
<div class="paragraph">
<div class="title">Operators within the Strimzi architecture</div>
<p><span class="image"><img src="images/operators.png" alt="Operators within the Strimzi architecture"></span></p>
</div>
<div class="sect2">
<h3 id="overview-components-cluster-operator-str"><a class="link" href="#overview-components-cluster-operator-str">4.1. Cluster Operator</a></h3>
<div class="paragraph">
<p>Strimzi uses the Cluster Operator to deploy and manage clusters for:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Kafka (including ZooKeeper, Entity Operator, Kafka Exporter, and Cruise Control)</p>
</li>
<li>
<p>Kafka Connect</p>
</li>
<li>
<p>Kafka MirrorMaker</p>
</li>
<li>
<p>Kafka Bridge</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Custom resources are used to deploy the clusters.</p>
</div>
<div class="paragraph">
<p>For example, to deploy a Kafka cluster:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>A <code>Kafka</code> resource with the cluster configuration is created within the Kubernetes cluster.</p>
</li>
<li>
<p>The Cluster Operator deploys a corresponding Kafka cluster, based on what is declared in the <code>Kafka</code> resource.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The Cluster Operator can also deploy (through configuration of the <code>Kafka</code> resource):</p>
</div>
<div class="ulist">
<ul>
<li>
<p>A Topic Operator to provide operator-style topic management through <code>KafkaTopic</code> custom resources</p>
</li>
<li>
<p>A User Operator to provide operator-style user management through <code>KafkaUser</code> custom resources</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The Topic Operator and User Operator function within the Entity Operator on deployment.</p>
</div>
<div class="paragraph">
<div class="title">Example architecture for the Cluster Operator</div>
<p><span class="image"><img src="images/cluster-operator.png" alt="The Cluster Operator creates and deploys Kafka and ZooKeeper clusters"></span></p>
</div>
</div>
<div class="sect2">
<h3 id="overview-concepts-topic-operator-str"><a class="link" href="#overview-concepts-topic-operator-str">4.2. Topic Operator</a></h3>
<div class="paragraph">
<p>The Topic Operator provides a way of managing topics in a Kafka cluster through Kubernetes resources.</p>
</div>
<div class="paragraph">
<div class="title">Example architecture for the Topic Operator</div>
<p><span class="image"><img src="images/topic-operator.png" alt="The Topic Operator manages topics for a Kafka cluster via KafkaTopic resources"></span></p>
</div>
<div class="paragraph">
<p>The role of the Topic Operator is to keep a set of <code>KafkaTopic</code> Kubernetes resources describing Kafka topics in-sync with corresponding Kafka topics.</p>
</div>
<div class="paragraph">
<p>Specifically, if a <code>KafkaTopic</code> is:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Created, the Topic Operator creates the topic</p>
</li>
<li>
<p>Deleted, the Topic Operator deletes the topic</p>
</li>
<li>
<p>Changed, the Topic Operator updates the topic</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Working in the other direction, if a topic is:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Created within the Kafka cluster, the Operator creates a <code>KafkaTopic</code></p>
</li>
<li>
<p>Deleted from the Kafka cluster, the Operator deletes the <code>KafkaTopic</code></p>
</li>
<li>
<p>Changed in the Kafka cluster, the Operator updates the <code>KafkaTopic</code></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>This allows you to declare a <code>KafkaTopic</code> as part of your application&#8217;s deployment and the Topic Operator will take care of creating the topic for you.
Your application just needs to deal with producing or consuming from the necessary topics.</p>
</div>
<div class="paragraph">
<p>If the topic is reconfigured or reassigned to different Kafka nodes, the <code>KafkaTopic</code> will always be up to date.</p>
</div>
</div>
<div class="sect2">
<h3 id="overview-concepts-user-operator-str"><a class="link" href="#overview-concepts-user-operator-str">4.3. User Operator</a></h3>
<div class="paragraph">
<p>The User Operator manages Kafka users for a Kafka cluster by watching for <code>KafkaUser</code> resources that describe Kafka users,
and ensuring that they are configured properly in the Kafka cluster.</p>
</div>
<div class="paragraph">
<p>For example, if a <code>KafkaUser</code> is:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Created, the User Operator creates the user it describes</p>
</li>
<li>
<p>Deleted, the User Operator deletes the user it describes</p>
</li>
<li>
<p>Changed, the User Operator updates the user it describes</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Unlike the Topic Operator, the User Operator does not sync any changes from the Kafka cluster with the Kubernetes resources.
Kafka topics can be created by applications directly in Kafka, but it is not expected that the users will be managed directly in the Kafka cluster in parallel with the User Operator.</p>
</div>
<div class="paragraph">
<p>The User Operator allows you to declare a <code>KafkaUser</code> resource as part of your application&#8217;s deployment.
You can specify the authentication and authorization mechanism for the user.
You can also configure <em>user quotas</em> that control usage of Kafka resources to ensure, for example, that a user does not monopolize access to a broker.</p>
</div>
<div class="paragraph">
<p>When the user is created, the user credentials are created in a <code>Secret</code>.
Your application needs to use the user and its credentials for authentication and to produce or consume messages.</p>
</div>
<div class="paragraph">
<p>In addition to managing credentials for authentication, the User Operator also manages authorization rules by including a description of the user&#8217;s access rights in the <code>KafkaUser</code> declaration.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="configuration-points_str"><a class="link" href="#configuration-points_str">5. Kafka configuration</a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>A deployment of Kafka components to a Kubernetes cluster using Strimzi is highly configurable through the application of custom resources.
Custom resources are created as instances of APIs added by Custom resource definitions (CRDs) to extend Kubernetes resources.</p>
</div>
<div class="paragraph">
<p>CRDs act as configuration instructions to describe the custom resources in a Kubernetes cluster,
and are provided with Strimzi for each Kafka component used in a deployment, as well as users and topics.
CRDs and custom resources are defined as YAML files.
Example YAML files are provided with the Strimzi distribution.</p>
</div>
<div class="paragraph">
<p>CRDs also allow Strimzi resources to benefit from native Kubernetes features like CLI accessibility and configuration validation.</p>
</div>
<div class="paragraph">
<p>In this chapter we look at how Kafka components are configured through custom resources, starting with common configuration points and then important configuration considerations specific to components.</p>
</div>
<div class="sect2">
<h3 id="configuration-points-resources_str"><a class="link" href="#configuration-points-resources_str">5.1. Custom resources</a></h3>
<div class="paragraph">
<p>After a new custom resource type is added to your cluster by installing a CRD, you can create instances of the resource based on its specification.</p>
</div>
<div class="paragraph">
<p>The custom resources for Strimzi components have common configuration properties, which are defined under <code>spec</code>.</p>
</div>
<div class="paragraph">
<p>In this fragment from a Kafka topic custom resource, the <code>apiVersion</code> and <code>kind</code> properties identify the associated CRD.
The <code>spec</code> property shows configuration that defines the number of partitions and replicas for the topic.</p>
</div>
<h4 id="kafka_topic_custom_resource" class="discrete">Kafka topic custom resource</h4>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: kafka.strimzi.io/v1beta1
kind: KafkaTopic
metadata:
  name: my-topic
  labels:
    strimzi.io/cluster: my-cluster
spec:
  partitions: 1
  replicas: 1
  # ...</code></pre>
</div>
</div>
<div class="paragraph">
<p>There are many additional configuration options that can be incorporated into a YAML definition, some common and some specific to a particular component.</p>
</div>
</div>
<div class="sect2">
<h3 id="configuration-points-common_str"><a class="link" href="#configuration-points-common_str">5.2. Common configuration</a></h3>
<div class="paragraph">
<p>Some of the configuration options common to resources are described here.
<a href="#security-overview_str">Security</a> and <a href="#metrics-overview_str">metrics collection</a> might also be adopted where applicable.</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Bootstrap servers</dt>
<dd>
<p>Bootstrap servers are used for host/port connection to a Kafka cluster for:</p>
<div class="ulist">
<ul>
<li>
<p>Kafka Connect</p>
</li>
<li>
<p>Kafka Bridge</p>
</li>
<li>
<p>Kafka MirrorMaker producers and consumers</p>
</li>
</ul>
</div>
</dd>
<dt class="hdlist1">CPU and memory resources</dt>
<dd>
<p>You request CPU and memory resources for components. Limits specify the maximum resources that can be consumed by a given container.</p>
<div class="paragraph">
<p>Resource requests and limits for the Topic Operator and User Operator are set in the <code>Kafka</code> resource.</p>
</div>
</dd>
<dt class="hdlist1">Logging</dt>
<dd>
<p>You define the logging level for the component. Logging can be defined directly (inline) or externally using a config map.</p>
</dd>
<dt class="hdlist1">Healthchecks</dt>
<dd>
<p>Healthcheck configuration introduces <em>liveness</em> and <em>readiness</em> probes to know when to restart a container (liveness) and when a container can accept traffic (readiness).</p>
</dd>
<dt class="hdlist1">JVM options</dt>
<dd>
<p>JVM options provide maximum and minimum memory allocation to optimize the performance of the component according to the platform it is running on.</p>
</dd>
<dt class="hdlist1">Pod scheduling</dt>
<dd>
<p>Pod schedules use <em>affinity/anti-affinity</em> rules to determine under what circumstances a pod is scheduled onto a node.</p>
</dd>
</dl>
</div>
<h4 id="example_yaml_showing_common_configuration" class="discrete">Example YAML showing common configuration</h4>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: kafka.strimzi.io/v1beta1
kind: KafkaConnect
metadata:
  name: my-cluster
spec:
  # ...
  bootstrapServers: my-cluster-kafka-bootstrap:9092
  resources:
    requests:
      cpu: 12
      memory: 64Gi
    limits:
      cpu: 12
      memory: 64Gi
  logging:
    type: inline
    loggers:
      connect.root.logger.level: "INFO"
  readinessProbe:
    initialDelaySeconds: 15
    timeoutSeconds: 5
  livenessProbe:
    initialDelaySeconds: 15
    timeoutSeconds: 5
  jvmOptions:
    "-Xmx": "2g"
    "-Xms": "2g"
  template:
    pod:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: node-type
                    operator: In
                    values:
                      - fast-network
  # ...</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="configuration-points-broker_str"><a class="link" href="#configuration-points-broker_str">5.3. Kafka cluster configuration</a></h3>
<div class="paragraph">
<p>A kafka cluster comprises one or more brokers.
For producers and consumers to be able to access topics within the brokers, Kafka configuration must define how data is stored in the cluster, and how the data is accessed.
You can configure a Kafka cluster to run with multiple broker nodes across <em>racks</em>.</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Storage</dt>
<dd>
<p>Kafka and ZooKeeper store data on disks.</p>
<div class="paragraph">
<p>Strimzi requires block storage provisioned through <code>StorageClass</code>.
The file system format for storage must be <em>XFS</em> or <em>EXT4</em>.
Three types of data storage are supported:</p>
</div>
<div class="openblock">
<div class="content">
<div class="dlist">
<dl>
<dt class="hdlist1">Ephemeral (Recommended for development only)</dt>
<dd>
<p>Ephemeral storage stores data for the lifetime of an instance. Data is lost when the instance is restarted.</p>
</dd>
<dt class="hdlist1">Persistent</dt>
<dd>
<p>Persistent storage relates to long-term data storage independent of the lifecycle of the instance.</p>
</dd>
<dt class="hdlist1">JBOD (Just a Bunch of Disks, suitable for Kafka only)</dt>
<dd>
<p>JBOD allows you to use multiple disks to store commit logs in each broker.</p>
</dd>
</dl>
</div>
</div>
</div>
<div class="paragraph">
<p>The disk capacity used by an existing Kafka cluster can be increased if supported by the infrastructure.</p>
</div>
</dd>
<dt class="hdlist1">Listeners</dt>
<dd>
<p>Listeners configure how clients connect to a Kafka cluster.</p>
<div class="paragraph">
<p>By specifying a unique name and port for each listener within a Kafka cluster,
you can configure multiple listeners.</p>
</div>
<div class="paragraph">
<p>The following types of listener are supported:</p>
</div>
<div class="openblock">
<div class="content">
<div class="ulist">
<ul>
<li>
<p><strong>Internal listeners</strong> for access within Kubernetes</p>
</li>
<li>
<p><strong>External listeners</strong> for access outside of Kubernetes</p>
</li>
</ul>
</div>
</div>
</div>
<div class="paragraph">
<p>You can enable TLS encryption for listeners, and configure <a href="#security-configuration-authentication_str">authentication</a>.</p>
</div>
<div class="paragraph">
<p>Internal listeners are specified using an <code>internal</code> type.</p>
</div>
<div class="paragraph">
<p>External listeners expose Kafka by specifying an external <code>type</code>:</p>
</div>
<div class="openblock">
<div class="content">
<div class="ulist">
<ul>
<li>
<p><code>route</code> to use OpenShift routes and the default HAProxy router</p>
</li>
<li>
<p><code>loadbalancer</code> to use loadbalancer services</p>
</li>
<li>
<p><code>nodeport</code> to use ports on Kubernetes nodes</p>
</li>
<li>
<p><code>ingress</code> to use Kubernetes <em>Ingress</em> and the <a href="https://github.com/kubernetes/ingress-nginx" target="_blank" rel="noopener">NGINX Ingress Controller for Kubernetes</a></p>
</li>
</ul>
</div>
</div>
</div>
</dd>
</dl>
</div>
<div class="paragraph">
<p>If you are using <a href="#security-configuration-authentication_str">OAuth 2.0 for token-based authentication</a>, you can configure listeners to use the authorization server.</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Rack awareness</dt>
<dd>
<p>Rack awareness is a configuration feature that distributes Kafka broker pods and topic replicas across <em>racks</em>, which represent data centers or racks in data centers, or availability zones.</p>
</dd>
</dl>
</div>
<h4 id="example_yaml_showing_kafka_configuration" class="discrete">Example YAML showing Kafka configuration</h4>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">apiVersion: kafka.strimzi.io/v1beta1
kind: Kafka
metadata:
  name: my-cluster
spec:
  kafka:
    # ...
    listeners:
      - name: tls
        port: 9093
        type: internal
        tls: true
        authentication:
          type: tls
      - name: external1
        port: 9094
        type: route
        tls: true
        authentication:
          type: tls
    # ...
    storage:
      type: persistent-claim
      size: 10000Gi
    # ...
    rack:
      topologyKey: topology.kubernetes.io/zone
    # ...</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="configuration-points-topic_str"><a class="link" href="#configuration-points-topic_str">5.4. Kafka MirrorMaker configuration</a></h3>
<div class="paragraph">
<p>To set up MirrorMaker, a source and target (destination) Kafka cluster must be running.</p>
</div>
<div class="paragraph">
<p>You can use Strimzi with MirrorMaker 2.0, although the earlier version of MirrorMaker continues to be supported.</p>
</div>
<h4 id="mirrormaker_2_0" class="discrete">MirrorMaker 2.0</h4>
<div class="paragraph">
<p>MirrorMaker 2.0 is based on the Kafka Connect framework, <em>connectors</em> managing the transfer of data between clusters.</p>
</div>
<div class="paragraph">
<p>MirrorMaker 2.0 uses:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Source cluster configuration to consume data from the source cluster</p>
</li>
<li>
<p>Target cluster configuration to output data to the target cluster</p>
</li>
</ul>
</div>
<h5 id="cluster_configuration" class="discrete">Cluster configuration</h5>
<div class="paragraph">
<p>You can use MirrorMaker 2.0 in <em>active/passive</em> or <em>active/active</em> cluster configurations.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>In an <em>active/active</em> configuration, both clusters are active and provide the same data simultaneously, which is useful if you want to make the same data available locally in different geographical locations.</p>
</li>
<li>
<p>In an <em>active/passive</em> configuration, the data from an active cluster is replicated in a passive cluster, which remains on standby, for example, for data recovery in the event of system failure.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>You configure a <code>KafkaMirrorMaker2</code> custom resource to define the Kafka Connect deployment, including the connection details of the source and target clusters,
and then run a set of MirrorMaker 2.0 connectors to make the connection.</p>
</div>
<div class="paragraph">
<p>Topic configuration is automatically synchronized between the source and target clusters according to the topics defined in the <code>KafkaMirrorMaker2</code> custom resource.
Configuration changes are propagated to remote topics so that new topics and partitions are detected and created.
Topic replication is defined using regular expression patterns to <em>whitelist</em> or <em>blacklist</em> topics.</p>
</div>
<div class="paragraph">
<p>The following MirrorMaker 2.0 connectors and related internal topics help manage the transfer and synchronization of data between the clusters.</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">MirrorSourceConnector</dt>
<dd>
<p>A <em>MirrorSourceConnector</em> creates remote topics from the source cluster.</p>
</dd>
<dt class="hdlist1">MirrorCheckpointConnector</dt>
<dd>
<p>A <em>MirrorCheckpointConnector</em> tracks and maps offsets for specified consumer groups using an <em>offset sync</em> topic and <em>checkpoint</em> topic.
The offset sync topic maps the source and target offsets for replicated topic partitions from record metadata.
A checkpoint is emitted from each source cluster and replicated in the target cluster through the checkpoint topic.
The checkpoint topic maps the last committed offset in the source and target cluster for replicated topic partitions in each consumer group.</p>
</dd>
<dt class="hdlist1">MirrorHeartbeatConnector</dt>
<dd>
<p>A <em>MirrorHeartbeatConnector</em> periodically checks connectivity between clusters.
A heartbeat is produced every second by the MirrorHeartbeatConnector into a <em>heartbeat</em> topic that is created on the local cluster.
If you have MirrorMaker 2.0 at both the remote and local locations, the heartbeat emitted at the remote location by the MirrorHeartbeatConnector is treated like any remote topic and mirrored by the MirrorSourceConnector at the local cluster.
The heartbeat topic makes it easy to check that the remote cluster is available and the clusters are connected.
If things go wrong, the heartbeat topic offset positions and time stamps can help with recovery and diagnosis.</p>
</dd>
</dl>
</div>
<div class="imageblock">
<div class="content">
<img src="images/mirrormaker.png" alt="MirrorMaker 2.0 replication between a Kafka cluster in Region 1 and a Kafka cluster in Region 2">
</div>
<div class="title">Figure 1. Replication across two clusters</div>
</div>
<h5 id="bidirectional_replication_across_two_clusters" class="discrete">Bidirectional replication across two clusters</h5>
<div class="paragraph">
<p>The MirrorMaker 2.0 architecture supports bidirectional replication in an <em>active/active</em> cluster configuration,
so both clusters are active and provide the same data simultaneously.
A MirrorMaker 2.0 cluster is required at each target destination.</p>
</div>
<div class="paragraph">
<p>Remote topics are distinguished by automatic renaming that prepends the name of cluster to the name of the topic.
This is useful if you want to make the same data available locally in different geographical locations.</p>
</div>
<div class="paragraph">
<p>However, if you want to backup or migrate data in an active/passive cluster configuration, you might want to keep the original names of the topics.
If so, you can configure MirrorMaker 2.0 to turn off automatic renaming.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="images/mirrormaker-renaming.png" alt="MirrorMaker 2.0 bidirectional architecture">
</div>
<div class="title">Figure 2. Bidirectional replication</div>
</div>
<h5 id="example_yaml_showing_mirrormaker_2_0_configuration" class="discrete">Example YAML showing MirrorMaker 2.0 configuration</h5>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">  apiVersion: kafka.strimzi.io/v1alpha1
  kind: KafkaMirrorMaker2
  metadata:
    name: my-mirror-maker2
    spec:
      version: 2.7.0
      connectCluster: "my-cluster-target"
      clusters:
      - alias: "my-cluster-source"
        bootstrapServers: my-cluster-source-kafka-bootstrap:9092
      - alias: "my-cluster-target"
        bootstrapServers: my-cluster-target-kafka-bootstrap:9092
      mirrors:
      - sourceCluster: "my-cluster-source"
        targetCluster: "my-cluster-target"
        sourceConnector: {}
      topicsPattern: ".*"
      groupsPattern: "group1|group2|group3"</code></pre>
</div>
</div>
<h4 id="mirrormaker" class="discrete">MirrorMaker</h4>
<div class="paragraph">
<p>The earlier version of MirrorMaker uses producers and consumers to replicate data across clusters.</p>
</div>
<div class="paragraph">
<p>MirrorMaker uses:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Consumer configuration to consume data from the source cluster</p>
</li>
<li>
<p>Producer configuration to output data to the target cluster</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Consumer and producer configuration includes any authentication and encryption settings.</p>
</div>
<div class="paragraph">
<p>A <em>whitelist</em> defines the topics to mirror from a source to a target cluster.</p>
</div>
<h5 id="key_consumer_configuration" class="discrete">Key Consumer configuration</h5>
<div class="dlist">
<dl>
<dt class="hdlist1">Consumer group identifier</dt>
<dd>
<p>The consumer group ID for a MirrorMaker consumer so that messages consumed are assigned to a consumer group.</p>
</dd>
<dt class="hdlist1">Number of consumer streams</dt>
<dd>
<p>A value to determine the number of consumers in a consumer group that consume a message in parallel.</p>
</dd>
<dt class="hdlist1">Offset commit interval</dt>
<dd>
<p>An offset commit interval to set the time between consuming and committing a message.</p>
</dd>
</dl>
</div>
<h5 id="key_producer_configuration" class="discrete">Key Producer configuration</h5>
<div class="dlist">
<dl>
<dt class="hdlist1">Cancel option for send failure</dt>
<dd>
<p>You can define whether a message send failure is ignored or MirrorMaker is terminated and recreated.</p>
</dd>
</dl>
</div>
<h5 id="example_yaml_showing_mirrormaker_configuration" class="discrete">Example YAML showing MirrorMaker configuration</h5>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: kafka.strimzi.io/v1beta1
kind: KafkaMirrorMaker
metadata:
  name: my-mirror-maker
spec:
  # ...
  consumer:
    bootstrapServers: my-source-cluster-kafka-bootstrap:9092
    groupId: "my-group"
    numStreams: 2
    offsetCommitInterval: 120000
    # ...
  producer:
    # ...
    abortOnSendFailure: false
    # ...
  whitelist: "my-topic|other-topic"
  # ...</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="configuration-points-connect_str"><a class="link" href="#configuration-points-connect_str">5.5. Kafka Connect configuration</a></h3>
<div class="paragraph">
<p>A basic Kafka Connect configuration requires a bootstrap address to connect to a Kafka cluster, and encryption and authentication details.</p>
</div>
<div class="paragraph">
<p>Kafka Connect instances are configured by default with the same:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Group ID for the Kafka Connect cluster</p>
</li>
<li>
<p>Kafka topic to store the connector offsets</p>
</li>
<li>
<p>Kafka topic to store connector and task status configurations</p>
</li>
<li>
<p>Kafka topic to store connector and task status updates</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>If multiple different Kafka Connect instances are used, these settings must reflect each instance.</p>
</div>
<h4 id="example_yaml_showing_kafka_connect_configuration" class="discrete">Example YAML showing Kafka Connect configuration</h4>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: kafka.strimzi.io/v1beta1
kind: KafkaConnect
metadata:
  name: my-connect
spec:
  # ...
  config:
    group.id: my-connect-cluster
    offset.storage.topic: my-connect-cluster-offsets
    config.storage.topic: my-connect-cluster-configs
    status.storage.topic: my-connect-cluster-status
  # ...</code></pre>
</div>
</div>
<h4 id="connectors" class="discrete">Connectors</h4>
<div class="paragraph">
<p>Connectors are configured separately from Kafka Connect.
The configuration describes the source input data and target output data to feed into and out of Kafka Connect.
The external source data must reference specific topics that will store the messages.</p>
</div>
<div class="paragraph">
<p>Kafka provides two built-in connectors:</p>
</div>
<div class="openblock">
<div class="content">
<div class="ulist">
<ul>
<li>
<p><code>FileStreamSourceConnector</code> streams data from an external system to Kafka, reading lines from an input source and sending each line to a Kafka topic.</p>
</li>
<li>
<p><code>FileStreamSinkConnector</code> streams data from Kafka to an external system, reading messages from a Kafka topic and creating a line for each in an output file.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="paragraph">
<p>You can add other connectors using connector plugins, which are a set of JAR files or TGZ archives that define the implementation required to connect to certain types of external system.</p>
</div>
<div class="paragraph">
<p>You create a custom Kafka Connect image that uses new connector plugins.</p>
</div>
<div class="paragraph">
<p>To create the image, you can use:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Kafka Connect configuration so that Strimzi creates the new image automatically.</p>
</li>
<li>
<p>A Kafka container image on <a href="https://quay.io/organization/strimzi" target="_blank" rel="noopener">Container Registry</a> as a base image.</p>
</li>
<li>
<p>OpenShift <a href="https://docs.okd.io/3.11/dev_guide/builds/index.html" target="_blank" rel="noopener">builds</a> and the <a href="https://docs.okd.io/3.11/creating_images/s2i.html" target="_blank" rel="noopener">Source-to-Image (S2I)</a> framework to create new container images.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>For Strimzi to create the new image automatically, a <code>build</code> configuration requires <code>output</code> properties to reference a container registry that stores the container image,
and <code>plugins</code> properties to list the connector plugins and their artifacts to add to the image.</p>
</div>
<div class="paragraph">
<p>The <code>output</code> properties describe the type and name of the image, and optionally the name of the Secret containing the credentials needed to access the container registry.
The <code>plugins</code> properties describe the type of artifact and the URL from which the artifact is downloaded. Additionally, you can specify a SHA-512 checksum to verify the artifact before unpacking it.</p>
</div>
<div class="listingblock">
<div class="title">Example Kafka Connect configuration to create a new image automatically</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">apiVersion: kafka.strimzi.io/v1beta1
kind: KafkaConnect
metadata:
  name: my-connect-cluster
spec:
  # ...
  build:
    output:
      type: docker
      image: my-registry.io/my-org/my-connect-cluster:latest
      pushSecret: my-registry-credentials
    plugins:
      - name: debezium-postgres-connector
        artifacts:
          - type: tgz
            url: https://<em>ARTIFACT-ADDRESS</em>.tgz
            sha512sum: <em>HASH-NUMBER-TO-VERIFY-ARTIFACT</em>
      # ...
  #...</code></pre>
</div>
</div>
<h4 id="managing_connectors" class="discrete">Managing connectors</h4>
<div class="paragraph">
<p>You can use the KafkaConnector resource or the <a href="https://kafka.apache.org/documentation/#connect_rest">Kafka Connect REST API</a> to create and manage connector instances in a Kafka Connect cluster.
The KafkaConnector resource offers a Kubernetes-native approach, and is managed by the Cluster Operator.</p>
</div>
<div class="paragraph">
<p>The <code>spec</code> for the KafkaConnector resource specifies the connector class and configuration settings, as well as the maximum number of connector <em>tasks</em> to handle the data.</p>
</div>
<h4 id="example_yaml_showing_kafkaconnector_configuration" class="discrete">Example YAML showing KafkaConnector configuration</h4>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: kafka.strimzi.io/v1beta1
kind: KafkaConnector
metadata:
  name: my-source-connector
  labels:
    strimzi.io/cluster: my-connect-cluster
spec:
  class: org.apache.kafka.connect.file.FileStreamSourceConnector
  tasksMax: 2
  config:
    file: "/opt/kafka/LICENSE"
    topic: my-topic
    # ...</code></pre>
</div>
</div>
<div class="paragraph">
<p>You enable KafkaConnectors by adding an annotation to the <code>KafkaConnect</code> resource.
KafkaConnector resources must be deployed to the same namespace as the Kafka Connect cluster they link to.</p>
</div>
<h4 id="example_yaml_showing_annotation_to_enable_kafkaconnector" class="discrete">Example YAML showing annotation to enable KafkaConnector</h4>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: kafka.strimzi.io/v1beta1
kind: KafkaConnect
metadata:
  name: my-connect
  annotations:
    strimzi.io/use-connector-resources: "true"
  # ...</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="configuration-points-bridge_str"><a class="link" href="#configuration-points-bridge_str">5.6. Kafka Bridge configuration</a></h3>
<div class="paragraph">
<p>A Kafka Bridge configuration requires a bootstrap server specification for the Kafka cluster it connects to, as well as any encryption and authentication options required.</p>
</div>
<div class="paragraph">
<p>Kafka Bridge consumer and producer configuration is standard, as described in the <a href="http://kafka.apache.org/documentation/#consumerconfigs" target="_blank" rel="noopener">Apache Kafka configuration documentation for consumers</a> and <a href="http://kafka.apache.org/documentation/#producerconfigs" target="_blank" rel="noopener">Apache Kafka configuration documentation for producers</a>.</p>
</div>
<div class="paragraph">
<p>HTTP-related configuration options set the port connection which the server listens on.</p>
</div>
<h4 id="cors" class="discrete">CORS</h4>
<div class="paragraph">
<p>The Kafka Bridge supports the use of Cross-Origin Resource Sharing (CORS).
CORS is a HTTP mechanism that allows browser access to selected resources from more than one origin, for example, resources on different domains.
If you choose to use CORS, you can define a list of allowed resource origins and HTTP methods for interaction with the Kafka cluster through the Kafka Bridge.
The lists are defined in the <code>http</code> specification of the Kafka Bridge configuration.</p>
</div>
<div class="paragraph">
<p>CORS allows for <em>simple</em> and <em>preflighted</em> requests between origin sources on different domains.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>A simple request is a HTTP request that must have an allowed origin defined in its header.</p>
</li>
<li>
<p>A preflighted request sends an initial OPTIONS HTTP request before the actual request to check that the origin and the method are allowed.</p>
</li>
</ul>
</div>
<h4 id="example_yaml_showing_kafka_bridge_configuration" class="discrete">Example YAML showing Kafka Bridge configuration</h4>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: kafka.strimzi.io/v1alpha1
kind: KafkaBridge
metadata:
  name: my-bridge
spec:
  # ...
  bootstrapServers: my-cluster-kafka:9092
  http:
    port: 8080
    cors:
      allowedOrigins: "https://strimzi.io"
      allowedMethods: "GET,POST,PUT,DELETE,OPTIONS,PATCH"
  consumer:
    config:
      auto.offset.reset: earliest
  producer:
    config:
      delivery.timeout.ms: 300000
  # ...</code></pre>
</div>
</div>
<div class="ulist">
<div class="title">Additional resources</div>
<ul>
<li>
<p><a href="https://www.w3.org/TR/cors/">Fetch</a> CORS specification</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="security-overview_str"><a class="link" href="#security-overview_str">6. Securing Kafka</a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>A secure deployment of Strimzi can encompass:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Encryption for data exchange</p>
</li>
<li>
<p>Authentication to prove identity</p>
</li>
<li>
<p>Authorization to allow or decline actions executed by users</p>
</li>
</ul>
</div>
<div class="sect2">
<h3 id="security-configuration-encryption_str"><a class="link" href="#security-configuration-encryption_str">6.1. Encryption</a></h3>
<div class="paragraph">
<p>Strimzi supports Transport Layer Security (TLS), a protocol for encrypted communication.</p>
</div>
<div class="paragraph">
<p>Communication is always encrypted for communication between:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Kafka brokers</p>
</li>
<li>
<p>ZooKeeper nodes</p>
</li>
<li>
<p>Operators and Kafka brokers</p>
</li>
<li>
<p>Operators and ZooKeeper nodes</p>
</li>
<li>
<p>Kafka Exporter</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>You can also configure TLS between Kafka brokers and clients by applying TLS encryption to the listeners of the Kafka broker.
TLS is specified for external clients when configuring an external listener.</p>
</div>
<div class="paragraph">
<p>Strimzi components and Kafka clients use digital certificates for encryption.
The Cluster Operator sets up certificates to enable encryption within the Kafka cluster.
You can provide your own server certificates, referred to as <em>Kafka listener certificates</em>,
for communication between Kafka clients and Kafka brokers, and inter-cluster communication.</p>
</div>
<div class="paragraph">
<p>Strimzi uses <em>Secrets</em> to store the certificates and private keys required for TLS in PEM and PKCS #12 format.</p>
</div>
<div class="paragraph">
<p>A TLS Certificate Authority (CA) issues certificates to authenticate the identity of a component.
Strimzi verifies the certificates for the components against the CA certificate.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Strimzi components are verified against the <em>cluster CA</em> Certificate Authority (CA)</p>
</li>
<li>
<p>Kafka clients are verified against the <em>clients CA</em> Certificate Authority (CA)</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="security-configuration-authentication_str"><a class="link" href="#security-configuration-authentication_str">6.2. Authentication</a></h3>
<div class="paragraph">
<p>Kafka listeners use authentication to ensure a secure client connection to the Kafka cluster.</p>
</div>
<div class="paragraph">
<p>Supported authentication mechanisms:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Mutual TLS client authentication (on listeners with TLS enabled encryption)</p>
</li>
<li>
<p>SASL SCRAM-SHA-512</p>
</li>
<li>
<p>OAuth 2.0 token based authentication</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The User Operator manages user credentials for TLS and SCRAM authentication, but not OAuth 2.0.
For example, through the User Operator you can create a user representing a client that requires access to the Kafka cluster, and specify TLS as the authentication type.</p>
</div>
<div class="paragraph">
<p>Using OAuth 2.0 token-based authentication, application clients can access Kafka brokers without exposing account credentials.
An authorization server handles the granting of access and inquiries about access.</p>
</div>
</div>
<div class="sect2">
<h3 id="security-configuration-authorization_str"><a class="link" href="#security-configuration-authorization_str">6.3. Authorization</a></h3>
<div class="paragraph">
<p>Kafka clusters use authorization to control the operations that are permitted on Kafka brokers by specific clients or users.
If applied to a Kafka cluster, authorization is enabled for all listeners used for client connection.</p>
</div>
<div class="paragraph">
<p>If a user is added to a list of <em>super users</em> in a Kafka broker configuration,
the user is allowed unlimited access to the cluster regardless of any authorization constraints implemented through authorization mechanisms.</p>
</div>
<div class="paragraph">
<p>Supported authorization mechanisms:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Simple authorization</p>
</li>
<li>
<p>OAuth 2.0 authorization (if you are using OAuth 2.0 token-based authentication)</p>
</li>
<li>
<p>Open Policy Agent (OPA) authorization</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Simple authorization uses <code>AclAuthorizer</code>, the default Kafka authorization plugin.
<code>AclAuthorizer</code> uses Access Control Lists (ACLs) to define which users have access to which resources.</p>
</div>
<div class="paragraph">
<p>OAuth 2.0 and OPA provide policy-based control from an authorization server.
Security policies and permissions used to grant access to resources on Kafka brokers are defined in the authorization server.</p>
</div>
<div class="paragraph">
<p>URLs are used to connect to the authorization server and verify that an operation requested by a client or user is allowed or denied.
Users and clients are matched against the policies created in the authorization server that permit access to perform specific actions on Kafka brokers.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="metrics-overview_str"><a class="link" href="#metrics-overview_str">7. Monitoring</a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>Monitoring data allows you to monitor the performance and health of Strimzi.
You can configure your deployment to capture metrics data for analysis and notifications.</p>
</div>
<div class="paragraph">
<p>Metrics data is useful when investigating issues with connectivity and data delivery.
For example, metrics data can identify under-replicated partitions or the rate at which messages are consumed.
Alerting rules can provide time-critical notifications on such metrics through a specified communications channel.
Monitoring visualizations present real-time metrics data to help determine when and how to update the configuration of your deployment.
Example metrics configuration files are provided with Strimzi.</p>
</div>
<div class="paragraph">
<p>Distributed tracing complements the gathering of metrics data by providing a facility for end-to-end tracking of messages through Strimzi.</p>
</div>
<div class="paragraph">
<p>Cruise Control provides support for rebalancing of Kafka clusters, based on workload data.</p>
</div>
<div class="paragraph">
<div class="title">Metrics and monitoring tools</div>
<p>Strimzi can employ the following tools for metrics and monitoring:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Prometheus</strong> pulls metrics from Kafka, ZooKeeper and Kafka Connect clusters. The Prometheus <strong>Alertmanager</strong> plugin handles alerts and routes them to a notification service.</p>
</li>
<li>
<p><strong>Kafka Exporter</strong> adds additional Prometheus metrics</p>
</li>
<li>
<p><strong>Grafana</strong> provides dashboard visualizations of Prometheus metrics</p>
</li>
<li>
<p><strong>Jaeger</strong> provides distributed tracing support to track transactions between applications</p>
</li>
<li>
<p><strong>Cruise Control</strong> balances data across a Kafka cluster</p>
</li>
</ul>
</div>
<div class="ulist">
<div class="title">Additional resources</div>
<ul>
<li>
<p><a href="https://github.com/prometheus" target="_blank" rel="noopener">Prometheus</a></p>
</li>
<li>
<p><a href="https://github.com/danielqsj/kafka_exporter" target="_blank" rel="noopener">Kafka Exporter</a></p>
</li>
<li>
<p><a href="https://grafana.com/" target="_blank" rel="noopener">Grafana Labs</a></p>
</li>
<li>
<p><a href="https://www.jaegertracing.io/" target="_blank" rel="noopener">Jaeger</a></p>
</li>
<li>
<p><a href="https://github.com/linkedin/cruise-control/wiki" target="_blank" rel="noopener">Cruise Control Wiki</a></p>
</li>
</ul>
</div>
<div class="sect2">
<h3 id="metrics-overview-tools-str"><a class="link" href="#metrics-overview-tools-str">7.1. Prometheus</a></h3>
<div class="paragraph">
<p>Prometheus can extract metrics data from Kafka components and the Strimzi Operators.</p>
</div>
<div class="paragraph">
<p>To use Prometheus to obtain metrics data and provide alerts, Prometheus and the Prometheus Alertmanager plugin must be deployed.
Kafka resources must also be deployed or redeployed with metrics configuration to expose the metrics data.</p>
</div>
<div class="paragraph">
<p>Prometheus scrapes the exposed metrics data for monitoring.
Alertmanager issues alerts when conditions indicate potential problems, based on pre-defined alerting rules.</p>
</div>
<div class="paragraph">
<p>Sample metrics and alerting rules configuration files are provided with Strimzi.
The sample alerting mechanism provided with Strimzi is configured to send notifications to a Slack channel.</p>
</div>
</div>
<div class="sect2">
<h3 id="metrics-overview-grafana_str"><a class="link" href="#metrics-overview-grafana_str">7.2. Grafana</a></h3>
<div class="paragraph">
<p>Grafana uses the metrics data exposed by Prometheus to present dashboard visualizations for monitoring.</p>
</div>
<div class="paragraph">
<p>A deployment of Grafana is required, with Prometheus added as a data source.
Example dashboards, supplied with Strimzi as JSON files, are imported through the Grafana interface to present monitoring data.</p>
</div>
</div>
<div class="sect2">
<h3 id="metrics-overview-exporter_str"><a class="link" href="#metrics-overview-exporter_str">7.3. Kafka Exporter</a></h3>
<div class="paragraph">
<p>Kafka Exporter is an open source project to enhance monitoring of Apache Kafka brokers and clients.
Kafka Exporter is deployed with a Kafka cluster to extract additional Prometheus metrics data from Kafka brokers related to offsets, consumer groups, consumer lag, and topics.
You can use the Grafana dashboard provided to visualize the data collected by Prometheus from Kafka Exporter.</p>
</div>
<div class="paragraph">
<p>A sample configuration file, alerting rules and Grafana dashboard for Kafka Exporter are provided with Strimzi.</p>
</div>
</div>
<div class="sect2">
<h3 id="metrics-overview-tracing_str"><a class="link" href="#metrics-overview-tracing_str">7.4. Distributed tracing</a></h3>
<div class="paragraph">
<p>Within a Kafka deployment, distributed tracing using Jaeger is supported for:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>MirrorMaker to trace messages from a source cluster to a target cluster</p>
</li>
<li>
<p>Kafka Connect to trace messages consumed and produced by Kafka Connect</p>
</li>
<li>
<p>Kafka Bridge to trace messages consumed and produced by Kafka Bridge, and HTTP requests from client applications</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Template configuration properties are set for the Kafka resources, which describe tracing environment variables.</p>
</div>
<h4 id="tracing_for_kafka_clients" class="discrete">Tracing for Kafka clients</h4>
<div class="paragraph">
<p>Client applications, such as Kafka producers and consumers, can also be set up so that transactions are monitored.
Clients are configured with a tracing profile, and a tracer is initialized for the client application to use.</p>
</div>
</div>
<div class="sect2">
<h3 id="metrics-overview-cruisecontrol_str"><a class="link" href="#metrics-overview-cruisecontrol_str">7.5. Cruise Control</a></h3>
<div class="paragraph">
<p>Cruise Control is an open source project for simplifying the monitoring and balancing of data across a Kafka cluster.
Cruise Control is deployed alongside a Kafka cluster to monitor its traffic, propose more balanced partition assignments, and trigger partition reassignments based on those proposals.</p>
</div>
<div class="paragraph">
<p>Cruise Control collects resource utilization information to model and analyze the workload of the Kafka cluster.
Based on <em>optimization goals</em> that have been defined, Cruise Control generates <em>optimization proposals</em> outlining how the cluster can be effectively rebalanced.
When an <em>optimization proposal</em> is approved, Cruise Control applies the rebalancing outlined in the proposal.</p>
</div>
<div class="paragraph">
<p>Prometheus can extract Cruise Control metrics data, including data related to optimization proposals and rebalancing operations.
A sample configuration file and Grafana dashboard for Cruise Control are provided with Strimzi.</p>
</div>
</div>
</div>
</div>