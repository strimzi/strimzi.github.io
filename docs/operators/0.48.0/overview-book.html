<div id="toc" class="toc">
<div id="toctitle">Table of Contents</div>
<ul class="sectlevel1">
<li><a href="#key-features_str">1. Key features</a>
<ul class="sectlevel2">
<li><a href="#kafka_capabilities">1.1. Kafka capabilities</a></li>
<li><a href="#kafka_use_cases">1.2. Kafka use cases</a></li>
<li><a href="#how_strimzi_supports_kafka">1.3. How Strimzi supports Kafka</a></li>
<li><a href="#why_use_strimzi_to_run_kafka_on_kubernetes">1.4. Why use Strimzi to run Kafka on Kubernetes?</a></li>
</ul>
</li>
<li><a href="#overview-components_str">2. Strimzi operators</a>
<ul class="sectlevel2">
<li><a href="#overview-components-cluster-operator-str">2.1. Cluster Operator</a></li>
<li><a href="#overview-concepts-topic-operator-str">2.2. Topic Operator</a></li>
<li><a href="#overview-concepts-user-operator-str">2.3. User Operator</a></li>
<li><a href="#overview-concepts-drain-cleaner-str">2.4. Drain Cleaner</a></li>
<li><a href="#con-feature-gates-overview-str">2.5. Feature gates in Strimzi Operators</a></li>
</ul>
</li>
<li><a href="#kafka-components_str">3. Strimzi deployment of Kafka</a>
<ul class="sectlevel2">
<li><a href="#kafka-deploy-options-str">3.1. Deployment options</a></li>
<li><a href="#kafka-concepts-node-pools-str">3.2. Node pools</a></li>
<li><a href="#kafka-concepts-components_str">3.3. Kafka component architecture</a></li>
</ul>
</li>
<li><a href="#kafka-concepts_str">4. About Kafka</a>
<ul class="sectlevel2">
<li><a href="#kafka-concepts-key_str">4.1. How Kafka operates as a message broker</a></li>
<li><a href="#kafka-concepts-producers-consumers_str">4.2. Producers and consumers</a></li>
</ul>
</li>
<li><a href="#kafka-connect-components_str">5. About Kafka Connect</a>
<ul class="sectlevel2">
<li><a href="#key-features-kafka-connect_str">5.1. How Kafka Connect streams data</a>
<ul class="sectlevel3">
<li><a href="#connectors">5.1.1. Connectors</a></li>
<li><a href="#tasks">5.1.2. Tasks</a></li>
<li><a href="#workers">5.1.3. Workers</a></li>
<li><a href="#transforms">5.1.4. Transforms</a></li>
<li><a href="#converters">5.1.5. Converters</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#con-overview-mm2-str">6. About MirrorMaker 2</a>
<ul class="sectlevel2">
<li><a href="#bidirectional_replication_activeactive">6.1. Bidirectional replication (active/active)</a></li>
<li><a href="#unidirectional_replication_activepassive">6.2. Unidirectional replication (active/passive)</a></li>
</ul>
</li>
<li><a href="#overview-components-kafka-bridge_str">7. Kafka Bridge interface</a>
<ul class="sectlevel2">
<li><a href="#http_requests">7.1. HTTP requests</a></li>
<li><a href="#con-overview-components-kafka-bridge-clients_str">7.2. Supported clients for the Kafka Bridge</a></li>
</ul>
</li>
<li><a href="#configuration-points_str">8. Configuring Kafka</a></li>
<li><a href="#security-overview_str">9. Securing Kafka</a></li>
<li><a href="#metrics-overview_str">10. Monitoring Kafka</a></li>
</ul>
</div>
<div class="sect1">
<h2 id="key-features_str"><a class="link" href="#key-features_str">1. Key features</a></h2>
<div class="sectionbody">
<div class="paragraph _abstract">
<p>Strimzi simplifies the process of running <a href="https://kafka.apache.org/" target="_blank" rel="noopener">Apache Kafka</a> within a Kubernetes cluster.</p>
</div>
<div class="paragraph">
<p>This guide serves as an introduction to Strimzi, outlining key Kafka concepts that are central to operating Strimzi.
It briefly explains Kafka&#8217;s components, their purposes, and configuration points, including security and monitoring options.
Strimzi provides the necessary files to deploy and manage a Kafka cluster, along with <a href="./deploying.html#config-examples-str" target="_blank" rel="noopener">example configuration files</a> for monitoring your deployment.</p>
</div>
<div class="sect2">
<h3 id="kafka_capabilities"><a class="link" href="#kafka_capabilities">1.1. Kafka capabilities</a></h3>
<div class="paragraph">
<p>Kafka&#8217;s data stream-processing capabilities and component architecture offer:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>High-throughput, low-latency data sharing for microservices and other applications</p>
</li>
<li>
<p>Guaranteed message ordering</p>
</li>
<li>
<p>Message rewind/replay from data storage to reconstruct application state</p>
</li>
<li>
<p>Message compaction to remove outdated records in a key-value log</p>
</li>
<li>
<p>Horizontal scalability within a cluster</p>
</li>
<li>
<p>Data replication to enhance fault tolerance</p>
</li>
<li>
<p>High-volume data retention for immediate access</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="kafka_use_cases"><a class="link" href="#kafka_use_cases">1.2. Kafka use cases</a></h3>
<div class="paragraph">
<p>Kafka&#8217;s capabilities make it ideal for:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Event-driven architectures</p>
</li>
<li>
<p>Event sourcing to log application state changes</p>
</li>
<li>
<p>Message brokering</p>
</li>
<li>
<p>Website activity tracking</p>
</li>
<li>
<p>Operational monitoring through metrics</p>
</li>
<li>
<p>Log collection and aggregation</p>
</li>
<li>
<p>Commit logs for distributed systems</p>
</li>
<li>
<p>Stream processing for real-time data responses</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="how_strimzi_supports_kafka"><a class="link" href="#how_strimzi_supports_kafka">1.3. How Strimzi supports Kafka</a></h3>
<div class="paragraph">
<p>Strimzi provides container images and operators for running Kafka on Kubernetes.
These operators are designed with specialized operational knowledge to efficiently manage Kafka on Kubernetes.</p>
</div>
<div class="paragraph">
<p>Strimzi operators simplify:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Deploying and running Kafka clusters</p>
</li>
<li>
<p>Deploying and managing Kafka components</p>
</li>
<li>
<p>Configuring Kafka access</p>
</li>
<li>
<p>Securing Kafka access</p>
</li>
<li>
<p>Upgrading Kafka</p>
</li>
<li>
<p>Managing brokers</p>
</li>
<li>
<p>Creating and managing topics</p>
</li>
<li>
<p>Creating and managing users</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>For detailed information and instructions on using operators to perform these operations, see the guide for <a href="./deploying.html" target="_blank" rel="noopener">Deploying and Managing Strimzi</a>.</p>
</div>
</div>
<div class="sect2">
<h3 id="why_use_strimzi_to_run_kafka_on_kubernetes"><a class="link" href="#why_use_strimzi_to_run_kafka_on_kubernetes">1.4. Why use Strimzi to run Kafka on Kubernetes?</a></h3>
<div class="paragraph">
<p>Running Kafka on Kubernetes without native support from Strimzi can be complex.
While deploying Kafka directly with standard resources like <code>StatefulSet</code> and <code>Service</code> is possible, the process is often error-prone and time-consuming.
This is especially true for operations like upgrades and configuration updates.
Strimzi dramatically reduces this complexity, providing the following advantages:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Native Kubernetes integration</dt>
<dd>
<p>Strimzi transforms Kafka into a Kubernetes-native application.
It extends the Kubernetes API with Custom Resources (CRs) like <code>Kafka</code>, <code>KafkaTopic</code>, and <code>KafkaUser</code>.
Custom resources offer a stable and highly configurable way to manage Kafka.
This allows you to define Kafka components at a high level, while the Strimzi operators automatically manage the underlying Kubernetes resources for you.
This native approach lowers the barrier to adoption and reduces operational overhead, making it easier to deploy and manage Kafka with less manual effort.</p>
</dd>
<dt class="hdlist1">Declarative cluster management</dt>
<dd>
<p>Manage the lifecycle of your Kafka resources declaratively.
Declarative control allows you to manage resources like topics and users directly in YAML.
This supports an Infrastructure-as-Code (IaC) workflow where Kafka configuration can be version-controlled, audited, and deployed through automated pipelines for a more consistent and repeatable setup.</p>
</dd>
<dt class="hdlist1">Support for upgrade, scaling, and recovery</dt>
<dd>
<p>Strimzi operators automate rolling upgrades and recovery of Kafka components, helping to reduce manual intervention and downtime.
They also support scaling of Kafka clusters through node pools, automated partition reassignment using Cruise Control to maintain balanced workloads, and safe node removal using the Strimzi Drain Cleaner.</p>
</dd>
<dt class="hdlist1">Integrated support for data streaming pipelines</dt>
<dd>
<p>When Strimzi is installed, you can deploy and manage Kafka clusters alongside supporting components such as Kafka Connect, MirrorMaker 2, and Kafka Bridge, all using Kubernetes-native custom resources.</p>
</dd>
<dt class="hdlist1">Integrated security</dt>
<dd>
<p>Strimzi enables fine-grained access control through listener-level authentication, cluster-wide authorization, and network policies.
It simplifies certificate management to support TLS encryption with configurable protocols and cipher suites, and manages secure client access through <code>KafkaUser</code> resources with ACLs, quotas, and handling of credentials.</p>
</dd>
</dl>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="overview-components_str"><a class="link" href="#overview-components_str">2. Strimzi operators</a></h2>
<div class="sectionbody">
<div class="paragraph _abstract">
<p>Operators are Kubernetes components that package, deploy, and manage applications by extending the Kubernetes API.
They simplify administrative tasks and reduce manual intervention.</p>
</div>
<div class="paragraph">
<p>Strimzi provides a set of operators to automate the deployment and management of Apache Kafka on Kubernetes.
Strimzi custom resources define the configuration for each deployment.</p>
</div>
<div class="paragraph">
<p>Strimzi includes the following operators:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Cluster Operator</dt>
<dd>
<p>Manages Kafka clusters and related components</p>
</dd>
<dt class="hdlist1">Topic Operator</dt>
<dd>
<p>Creates, configures, and deletes Kafka topics</p>
</dd>
<dt class="hdlist1">User Operator</dt>
<dd>
<p>Manages Kafka users and their authentication credentials</p>
</dd>
</dl>
</div>
<div class="paragraph">
<p>The Cluster Operator can deploy the Entity Operator, which runs the Topic Operator and User Operator in a single pod.
The Entity Operator can be configured to run one or both operators.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
The Entity Operator does not manage Kafka clusters.
It simply runs the Topic Operator and User Operator in separate containers within its pod, allowing them to handle topic and user management.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>For Kafka clusters not managed by Strimzi, the Topic Operator and User Operator can also be deployed standalone (without the Entity Operator).</p>
</div>
<div class="paragraph">
<p>Additionally, Strimzi provides Drain Cleaner, a separate tool that can be used alongside the Cluster Operator to assist with safe pod eviction during maintenance or upgrades.</p>
</div>
<div class="paragraph">
<div class="title">Operators within the Strimzi architecture</div>
<p><span class="image"><img src="images/operators.png" alt="Operators within the Strimzi architecture"></span></p>
</div>
<div class="sect2">
<h3 id="overview-components-cluster-operator-str"><a class="link" href="#overview-components-cluster-operator-str">2.1. Cluster Operator</a></h3>
<div class="paragraph _abstract">
<p>Strimzi uses the Cluster Operator to deploy and manage clusters.
By default, when you deploy Strimzi a single Cluster Operator replica is deployed.
You can add replicas with leader election so that additional Cluster Operators are on standby in case of disruption.</p>
</div>
<div class="paragraph">
<p>The Cluster Operator manages the clusters of the following Kafka components:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Kafka (including Entity Operator, Kafka Exporter, and Cruise Control)</p>
</li>
<li>
<p>Kafka Connect</p>
</li>
<li>
<p>Kafka MirrorMaker</p>
</li>
<li>
<p>Kafka Bridge</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The clusters are deployed using custom resources.</p>
</div>
<div class="paragraph">
<p>For example, to deploy a Kafka cluster:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>A <code>Kafka</code> resource with the cluster configuration is created within the Kubernetes cluster.</p>
</li>
<li>
<p>The Cluster Operator deploys a corresponding Kafka cluster, based on what is declared in the <code>Kafka</code> resource.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The Cluster Operator can also deploy the following Strimzi operators through configuration of the <code>Kafka</code> resource:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Topic Operator to provide operator-style topic management through <code>KafkaTopic</code> custom resources</p>
</li>
<li>
<p>User Operator to provide operator-style user management through <code>KafkaUser</code> custom resources</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The Topic Operator and User Operator function within the Entity Operator on deployment.</p>
</div>
<div class="paragraph">
<div class="title">Example architecture for the Cluster Operator</div>
<p><span class="image"><img src="images/cluster-operator.png" alt="The Cluster Operator creates and deploys Kafka clusters"></span></p>
</div>
</div>
<div class="sect2">
<h3 id="overview-concepts-topic-operator-str"><a class="link" href="#overview-concepts-topic-operator-str">2.2. Topic Operator</a></h3>
<div class="paragraph _abstract">
<p>The Topic Operator provides a way of managing topics in a Kafka cluster through <code>KafkaTopic</code> resources.
The Cluster Operator can deploy the Topic Operator as part of the <strong>Entity Operator</strong> configuration at the same time as a Kafka cluster.
The operator can also be deployed standalone to help manage topics for Kafka clusters not operated by Strimzi.</p>
</div>
<div class="paragraph">
<div class="title">Example architecture for the Topic Operator</div>
<p><span class="image"><img src="images/topic-operator.png" alt="The Topic Operator manages topics for a Kafka cluster via KafkaTopic resources"></span></p>
</div>
<div class="paragraph">
<p>The Topic Operator manages Kafka topics by watching for <code>KafkaTopic</code> resources that describe Kafka topics, and ensuring that they are configured properly in the Kafka cluster.</p>
</div>
<div class="paragraph">
<p>When a <code>KafkaTopic</code> is created, deleted, or changed, the Topic Operator performs the corresponding action on the Kafka topic.</p>
</div>
<div class="paragraph">
<p>You can declare a <code>KafkaTopic</code> as part of your application&#8217;s deployment and the Topic Operator manages the Kafka topic for you.</p>
</div>
</div>
<div class="sect2">
<h3 id="overview-concepts-user-operator-str"><a class="link" href="#overview-concepts-user-operator-str">2.3. User Operator</a></h3>
<div class="paragraph _abstract">
<p>The User Operator provides a way of managing users in a Kafka cluster through <code>KafkaUser</code> resources.
The Cluster Operator can deploy the User Operator as part of the <strong>Entity Operator</strong> configuration at the same time as a Kafka cluster.
The operator can also be deployed standalone to help manage users for Kafka clusters not operated by Strimzi.</p>
</div>
<div class="paragraph">
<p>The User Operator manages Kafka users for a Kafka cluster by watching for <code>KafkaUser</code> resources that describe Kafka users,
and ensuring that they are configured properly in the Kafka cluster.</p>
</div>
<div class="paragraph">
<p>When a <code>KafkaUser</code> is created, deleted, or changed, the User Operator performs the corresponding action on the Kafka user.</p>
</div>
<div class="paragraph">
<p>You can declare a <code>KafkaUser</code> resource as part of your application&#8217;s deployment and the User Operator manages the Kafka user for you.
You can specify the authentication and authorization mechanism for the user.
You can also configure <em>user quotas</em> that control usage of Kafka resources to ensure, for example, that a user does not monopolize access to a broker.</p>
</div>
<div class="paragraph">
<p>When the user is created, the user credentials are created in a <code>Secret</code>.
Your application needs to use the user and its credentials for authentication and to produce or consume messages.</p>
</div>
<div class="paragraph">
<p>In addition to managing credentials for authentication, the User Operator also manages authorization rules by including a description of the user&#8217;s access rights in the <code>KafkaUser</code> declaration.</p>
</div>
</div>
<div class="sect2">
<h3 id="overview-concepts-drain-cleaner-str"><a class="link" href="#overview-concepts-drain-cleaner-str">2.4. Drain Cleaner</a></h3>
<div class="paragraph _abstract">
<p>If you are using the Cluster Operator to manage your Kafka cluster, you can deploy and use the Drain Cleaner to streamline the process of moving Kafka pods from Kubernetes nodes scheduled for maintenance.</p>
</div>
<div class="paragraph">
<p>By deploying the Strimzi Drain Cleaner, you can use the Cluster Operator to move Kafka pods instead of Kubernetes.
Strimzi Drain Cleaner annotates pods being evicted with a rolling update annotation.
The annotation informs the Cluster Operator to perform the rolling update.</p>
</div>
<div class="paragraph">
<p>Drain Cleaner ensures that no partition replicas become under-replicated during node draining, maintaining data availability and fault tolerance.
This controlled approach minimizes potential disruptions to your Kafka cluster when draining pods.</p>
</div>
</div>
<div class="sect2">
<h3 id="con-feature-gates-overview-str"><a class="link" href="#con-feature-gates-overview-str">2.5. Feature gates in Strimzi Operators</a></h3>
<div class="paragraph _abstract">
<p>Strimzi operators use feature gates to enable or disable specific features and functions. Enabling a feature gate alters the behavior of the associated operator, introducing the corresponding feature to your Strimzi deployment.</p>
</div>
<div class="paragraph">
<p>Feature gates are set in the operator configuration and have three stages of maturity: alpha, beta, or graduated.
Graduated feature gates have reached General Availability (GA) and are permanently enabled features.</p>
</div>
<div class="paragraph">
<p>For more information, see <a href="./deploying.html#ref-operator-cluster-feature-gates-str" target="_blank" rel="noopener">Feature gates</a>.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="kafka-components_str"><a class="link" href="#kafka-components_str">3. Strimzi deployment of Kafka</a></h2>
<div class="sectionbody">
<div class="paragraph _abstract">
<p>Strimzi enables the deployment of Apache Kafka components to a Kubernetes cluster, typically running as clusters for high availability.</p>
</div>
<div class="paragraph">
<p>A standard Kafka deployment using Strimzi might include the following components:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Kafka</strong> cluster of broker nodes as the core component</p>
</li>
<li>
<p><strong>Kafka Connect</strong> cluster for external data connections</p>
</li>
<li>
<p><strong>Kafka MirrorMaker</strong> cluster to mirror data to another Kafka cluster</p>
</li>
<li>
<p><strong>Kafka Exporter</strong> to extract additional Kafka metrics data for monitoring</p>
</li>
<li>
<p><strong>Kafka Bridge</strong> to enable HTTP-based communication with Kafka</p>
</li>
<li>
<p><strong>Cruise Control</strong> to rebalance topic partitions across brokers</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Not all of these components are required, though you need Kafka as a minimum for a Strimzi-managed Kafka cluster.
Depending on your use case, you can deploy the additional components as needed.
These components can also be used with Kafka clusters that are not managed by Strimzi.</p>
</div>
<div class="sect2">
<h3 id="kafka-deploy-options-str"><a class="link" href="#kafka-deploy-options-str">3.1. Deployment options</a></h3>
<div class="paragraph _abstract">
<p>Strimzi can be deployed on Kubernetes using a method that aligns with how you configure and manage your clusters.</p>
</div>
<div class="paragraph">
<p>Choose from the following options:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Manual YAML deployment</strong>: Uses installation files to deploy all required resources, offering fine-tuned configuration and full flexibility.</p>
</li>
<li>
<p><strong>OperatorHub.io (OLM)</strong>:  Installs Strimzi through the Operator Lifecycle Manager (OLM) and simplifies lifecycle management.
Delivers a standard configuration that can be updated automatically or manually.</p>
</li>
<li>
<p><strong>Helm chart</strong>: Provides a streamlined and repeatable deployment approach using the Strimzi Helm chart.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>For detailed instructions, see the guide for <a href="./deploying.html" target="_blank" rel="noopener">Deploying and Managing Strimzi</a>.</p>
</div>
</div>
<div class="sect2">
<h3 id="kafka-concepts-node-pools-str"><a class="link" href="#kafka-concepts-node-pools-str">3.2. Node pools</a></h3>
<div class="paragraph _abstract">
<p>A Kafka cluster consists of nodes assigned KRaft roles.
These roles can be <em>brokers</em>, responsible for message streaming and storage, or <em>controllers</em>, which manage cluster state and metadata.
While a node can perform both roles, separating them in production simplifies cluster management.</p>
</div>
<div class="paragraph">
<p>Strimzi manages Kafka nodes using node pools.
A node pool is a distinct group of Kafka nodes within a cluster.
You define node pools using the <code>KafkaNodePool</code> custom resource.
Each node pool has its own configuration, defining aspects such as the role assigned to all its nodes, replica count, and storage settings.
Node pools are associated with a single Kafka cluster through this configuration.</p>
</div>
<div class="paragraph">
<p>Cluster-wide settings, such as the Kafka version and listener configuration, are defined in the <code>Kafka</code> custom resource.
Any configuration not defined in node pools is inherited from the cluster configuration in the <code>Kafka</code> resource.</p>
</div>
<div class="paragraph">
<p>The key benefits of using node pools are as follows:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Role separation</dt>
<dd>
<p>In production, it is recommended to run a dedicated node pool of three or more controller nodes, separate from broker nodes.
This separation isolates controllers from broker workloads and allows broker pools to scale independently.</p>
</dd>
<dt class="hdlist1">Distinct node pool configurations</dt>
<dd>
<p>Not all brokers need to be identical.
Node pools allow you to define different configurations within the same cluster, such as pools with fast or cost-effective storage to support different workloads.</p>
</dd>
<dt class="hdlist1">Simplified operations</dt>
<dd>
<p>Node pools simplify cluster operations such as scaling and hardware migration.
You can scale individual pools, move nodes between pools, or create new pools with updated storage configurations to replace older ones.</p>
</dd>
</dl>
</div>
</div>
<div class="sect2">
<h3 id="kafka-concepts-components_str"><a class="link" href="#kafka-concepts-components_str">3.3. Kafka component architecture</a></h3>
<div class="paragraph _abstract">
<p>A Kafka cluster consists of broker and controller nodes, which are managed using node pools.
Other Kafka components interact with the Kafka cluster to support specific tasks.</p>
</div>
<div class="paragraph">
<div class="title">Kafka component interaction</div>
<p><span class="image"><img src="images/overview/kafka-concepts-supporting-components.png" alt="Data flows between several Kafka components and the Kafka cluster."></span></p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Kafka Connect</dt>
<dd>
<p>Kafka Connect is an integration toolkit for streaming data between Kafka brokers and other systems using <em>connector</em> plugins.
Kafka Connect provides a framework for integrating Kafka with an external data source or target, such as a database, for import or export of data using connectors.
Connectors provide the connection configuration needed.</p>
<div class="openblock">
<div class="content">
<div class="ulist">
<ul>
<li>
<p>A <em>source</em> connector pushes external data into Kafka.</p>
</li>
<li>
<p>A <em>sink</em> connector extracts data  out of Kafka</p>
</li>
</ul>
</div>
</div>
</div>
<div class="paragraph">
<p>External data is translated and transformed into the appropriate format.</p>
</div>
<div class="paragraph">
<p>Kafka Connect can be configured to build custom container images with the required connectors.</p>
</div>
</dd>
<dt class="hdlist1">Kafka MirrorMaker</dt>
<dd>
<p>Kafka MirrorMaker replicates data between two Kafka clusters, either in the same data center or across different locations.</p>
</dd>
<dt class="hdlist1">Kafka Bridge</dt>
<dd>
<p>Kafka Bridge provides an API for integrating HTTP-based clients with a Kafka cluster.</p>
</dd>
<dt class="hdlist1">Kafka Exporter</dt>
<dd>
<p>Kafka Exporter extracts data for analysis as Prometheus metrics, primarily data relating to offsets, consumer groups, consumer lag and topics. Consumer lag is the delay between the last message written to a partition and the message currently being picked up from that partition by a consumer</p>
</dd>
</dl>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="kafka-concepts_str"><a class="link" href="#kafka-concepts_str">4. About Kafka</a></h2>
<div class="sectionbody">
<div class="paragraph _abstract">
<p>Apache Kafka is an open-source distributed publish-subscribe messaging system for fault-tolerant real-time data feeds.</p>
</div>
<div class="paragraph">
<p>For more information about Apache Kafka, see the <a href="https://kafka.apache.org/documentation/" target="_blank" rel="noopener">Apache Kafka documentation</a>.</p>
</div>
<div class="sect2">
<h3 id="kafka-concepts-key_str"><a class="link" href="#kafka-concepts-key_str">4.1. How Kafka operates as a message broker</a></h3>
<div class="paragraph _abstract">
<p>To maximise your experience of using Strimzi, you need to understand how Kafka operates as a message broker.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>A Kafka cluster comprises multiple nodes.</p>
</li>
<li>
<p>Nodes operating as brokers contain topics that receive and store data.</p>
</li>
<li>
<p>Topics are split by partitions, where the data is written.</p>
</li>
<li>
<p>Partitions are replicated across brokers for fault tolerance.</p>
</li>
</ul>
</div>
<div class="paragraph">
<div class="title">Kafka brokers and topics</div>
<p><span class="image"><img src="images/overview/kafka-concepts-key-concepts.png" alt="Kafka brokers and topics inside a Kafka cluster showing the partition leader of each topic"></span></p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Broker</dt>
<dd>
<p>A broker orchestrates the storage and passing of messages.</p>
</dd>
<dt class="hdlist1">Topic</dt>
<dd>
<p>A topic provides a destination for the storage of data.
Each topic is split into one or more partitions.</p>
</dd>
<dt class="hdlist1">Cluster</dt>
<dd>
<p>A group of broker instances.</p>
</dd>
<dt class="hdlist1">Partition</dt>
<dd>
<p>The number of topic partitions is defined by a topic <em>partition count</em>.</p>
</dd>
<dt class="hdlist1">Partition leader</dt>
<dd>
<p>A partition leader handles all producer requests for a topic.</p>
</dd>
<dt class="hdlist1">Partition follower</dt>
<dd>
<p>A partition follower replicates the partition data of a partition leader, optionally handling consumer requests.</p>
<div class="paragraph">
<p>Topics use a <em>replication factor</em> to configure the number of replicas of each partition within the cluster.
A topic comprises at least one partition.</p>
</div>
<div class="paragraph">
<p>An <em>in-sync</em> replica has the same number of messages as the leader.
Configuration defines how many replicas must be in-sync to be able to produce messages, ensuring that a message is committed only after it has been successfully copied to the replica partition.
In this way, if the leader fails the message is not lost.</p>
</div>
<div class="paragraph">
<p>In the <em>Kafka brokers and topics</em> diagram, we can see each numbered partition has a leader and two followers in replicated topics.</p>
</div>
</dd>
</dl>
</div>
</div>
<div class="sect2">
<h3 id="kafka-concepts-producers-consumers_str"><a class="link" href="#kafka-concepts-producers-consumers_str">4.2. Producers and consumers</a></h3>
<div class="paragraph _abstract">
<p>Producers and consumers send and receive messages (publish and subscribe) through brokers.
Messages comprise an optional <em>key</em> and a <em>value</em> that contains the message data, plus headers and related metadata.
The key is used to identify the subject of the message, or a property of the message.
Messages are delivered in batches, and batches and records contain headers and metadata that provide details that are useful for filtering and routing by clients, such as the timestamp and offset position for the record.</p>
</div>
<div class="paragraph">
<div class="title">Producers and consumers</div>
<p><span class="image"><img src="images/overview/kafka-concepts-producer-consumer.png" alt="A producer sends messages through a broker to a topic containing three partitions. Three consumers in a consumer group read the messages from the partitions"></span></p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Producer</dt>
<dd>
<p>A producer sends messages to a broker topic to be written to the end offset of a partition.
Messages are written to partitions by a producer on a round robin basis, or to a specific partition based on the message key.</p>
</dd>
<dt class="hdlist1">Consumer</dt>
<dd>
<p>A consumer subscribes to a topic and reads messages according to topic, partition and offset.</p>
</dd>
<dt class="hdlist1">Consumer group</dt>
<dd>
<p>Consumer groups are used to share a typically large data stream generated by multiple producers from a given topic.
Consumers are grouped using a <code>group.id</code>, allowing messages to be spread across the members.
Consumers within a group do not read data from the same partition, but can receive data from one or more partitions.</p>
</dd>
<dt class="hdlist1">Offsets</dt>
<dd>
<p>Offsets describe the position of messages within a partition.
Each message in a given partition has a unique offset, which helps identify the position of a consumer within the partition to track the number of records that have been consumed.</p>
<div class="paragraph">
<p>Committed offsets are written to an offset commit log.
A <code>__consumer_offsets</code> topic stores information on committed offsets, the position of last and next offset, according to consumer group.</p>
</div>
</dd>
</dl>
</div>
<div class="paragraph">
<div class="title">Producing and consuming data</div>
<p><span class="image"><img src="images/overview/kafka-concepts-partitions.png" alt="A producer sends a message to a broker topic; the message is written to the end offset (7). A consumer reads messages from offset 5"></span></p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="kafka-connect-components_str"><a class="link" href="#kafka-connect-components_str">5. About Kafka Connect</a></h2>
<div class="sectionbody">
<div class="paragraph _abstract">
<p>Kafka Connect is an integration toolkit for streaming data between Kafka brokers and other systems.
The other system is typically an external data source or target, such as a database.</p>
</div>
<div class="paragraph">
<p>Kafka Connect uses a plugin architecture to provide the implementation artifacts for connectors.
Plugins allow connections to other systems and provide additional configuration to manipulate data.
Plugins include connectors and other components, such as data converters and transforms.
A connector operates with a specific type of external system.
Each connector defines a schema for its configuration.
You supply the configuration to Kafka Connect to create a connector instance within Kafka Connect.
Connector instances then define a set of tasks for moving data between systems.</p>
</div>
<div class="paragraph">
<p>Plugins provide a set of one or more artifacts that define a connector and task implementation for connecting to a given kind of data source.
The configuration describes the source input data and target output data to feed into and out of Kafka Connect.
The plugins might also contain the libraries and files needed to transform the data.</p>
</div>
<div class="paragraph">
<p>A Kafka Connect deployment can have one or more plugins, but only one version of each plugin.
Plugins for many external systems are available for use with Kafka Connect.
You can also create your own plugins.</p>
</div>
<div class="paragraph">
<p>Strimzi operates Kafka Connect in <em>distributed mode</em>, distributing data streaming tasks across one or more worker pods.
A Kafka Connect cluster comprises a group of worker pods.
Each connector is instantiated on a single worker.
Each connector comprises one or more tasks that are distributed across the group of workers.
Distribution across workers permits highly scalable pipelines.</p>
</div>
<div class="paragraph">
<p>Workers convert data from one format into another format that&#8217;s suitable for the source or target system.
Depending on the configuration of the connector instance, workers might also apply transforms (also known as Single Message Transforms, or SMTs).
Transforms adjust messages, such as filtering certain data, before they are converted.
Kafka Connect has some built-in transforms, but other transformations can be provided by plugins if necessary.</p>
</div>
<div class="sect2">
<h3 id="key-features-kafka-connect_str"><a class="link" href="#key-features-kafka-connect_str">5.1. How Kafka Connect streams data</a></h3>
<div class="paragraph _abstract">
<p>Kafka Connect uses connector instances to integrate with other systems to stream data.</p>
</div>
<div class="paragraph">
<p>Kafka Connect loads existing connector instances on start up and distributes data streaming tasks and connector configuration across worker pods.
Workers run the tasks for the connector instances.
Each worker runs as a separate pod to make the Kafka Connect cluster more fault tolerant.
If there are more tasks than workers, workers are assigned multiple tasks.
If a worker fails, its tasks are automatically assigned to active workers in the Kafka Connect cluster.</p>
</div>
<div class="paragraph">
<p>The main Kafka Connect components used in streaming data are as follows:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Connectors to create tasks</p>
</li>
<li>
<p>Tasks to move data</p>
</li>
<li>
<p>Workers to run tasks</p>
</li>
<li>
<p>Transforms to manipulate data</p>
</li>
<li>
<p>Converters to convert data</p>
</li>
</ul>
</div>
<div class="sect3">
<h4 id="connectors"><a class="link" href="#connectors">5.1.1. Connectors</a></h4>
<div class="paragraph">
<p>Connectors can be one of the following type:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Source connectors that push data into Kafka</p>
</li>
<li>
<p>Sink connectors that extract data out of Kafka</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Plugins provide the implementation for Kafka Connect to run connector instances.
Connector instances create the tasks required to transfer data in and out of Kafka.
The Kafka Connect runtime orchestrates the tasks to split the work required between the worker pods.</p>
</div>
<div class="paragraph">
<p>MirrorMaker 2 also uses the Kafka Connect framework.
In this case, the external data system is another Kafka cluster.
Specialized connectors for MirrorMaker 2 manage data replication between source and target Kafka clusters.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>In addition to the MirrorMaker 2 connectors, Kafka provides two connectors as examples:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>FileStreamSourceConnector</code> streams data from a file on the worker&#8217;s filesystem to Kafka, reading the input file and sending each line to a given Kafka topic.</p>
</li>
<li>
<p><code>FileStreamSinkConnector</code> streams data from Kafka to the worker&#8217;s filesystem, reading messages from a Kafka topic and writing a line for each in an output file.</p>
</li>
</ul>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>The following source connector diagram shows the process flow for a source connector that streams records from an external data system.
A Kafka Connect cluster might operate source and sink connectors at the same time.
Workers are running in distributed mode in the cluster.
Workers can run one or more tasks for more than one connector instance.</p>
</div>
<div class="paragraph">
<div class="title">Source connector streaming data to Kafka</div>
<p><span class="image"><img src="images/overview/kafka-concepts-source-connector.png" alt="Kafka Connect source connector worker interaction in distributed mode"></span></p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>A plugin provides the implementation artifacts for the source connector</p>
</li>
<li>
<p>A single worker initiates the source connector instance</p>
</li>
<li>
<p>The source connector creates the tasks to stream data</p>
</li>
<li>
<p>Tasks run in parallel to poll the external data system and return records</p>
</li>
<li>
<p>Transforms adjust the records, such as filtering or relabelling them</p>
</li>
<li>
<p>Converters put the records into a format suitable for Kafka</p>
</li>
<li>
<p>The source connector is managed using KafkaConnectors or the Kafka Connect API</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>The following sink connector diagram shows the process flow when streaming data from Kafka to an external data system.</p>
</div>
<div class="paragraph">
<div class="title">Sink connector streaming data from Kafka</div>
<p><span class="image"><img src="images/overview/kafka-concepts-sink-connector.png" alt="Kafka Connect sink connector worker interaction in distributed mode"></span></p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>A plugin provides the implementation artifacts for the sink connector</p>
</li>
<li>
<p>A single worker initiates the sink connector instance</p>
</li>
<li>
<p>The sink connector creates the tasks to stream data</p>
</li>
<li>
<p>Tasks run in parallel to poll Kafka and return records</p>
</li>
<li>
<p>Converters put the records into a format suitable for the external data system</p>
</li>
<li>
<p>Transforms adjust the records, such as filtering or relabelling them</p>
</li>
<li>
<p>The sink connector is managed using KafkaConnectors or the Kafka Connect API</p>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="tasks"><a class="link" href="#tasks">5.1.2. Tasks</a></h4>
<div class="paragraph">
<p>Data transfer orchestrated by the Kafka Connect runtime is split into tasks that run in parallel.
A task is started using the configuration supplied by a connector instance.
Kafka Connect distributes the task configurations to workers, which instantiate and execute tasks.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>A source connector task polls the external data system and returns a list of records that a worker sends to the Kafka brokers.</p>
</li>
<li>
<p>A sink connector task receives Kafka records from a worker for writing to the external data system.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>For sink connectors, the number of tasks created relates to the number of partitions being consumed.
For source connectors, how the source data is partitioned is defined by the connector.
You can control the maximum number of tasks that can run in parallel by setting <code>tasksMax</code> in the connector configuration.
The connector might create fewer tasks than the maximum setting.
For example, the connector might create fewer tasks if it&#8217;s not possible to split the source data into that many partitions.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
In the context of Kafka Connect, a <em>partition</em> can mean a topic partition or a <em>shard of data</em> in an external system.
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="workers"><a class="link" href="#workers">5.1.3. Workers</a></h4>
<div class="paragraph">
<p>Workers employ the connector configuration deployed to the Kafka Connect cluster.
The configuration is stored in an internal Kafka topic used by Kafka Connect.
Workers also run connectors and their tasks.</p>
</div>
<div class="paragraph">
<p>A Kafka Connect cluster contains a group of workers with the same <code>group.id</code>.
The ID identifies the cluster within Kafka.
The ID is assigned in the worker configuration through the <code>KafkaConnect</code> resource.
Worker configuration also specifies the names of internal Kafka Connect topics.
The topics store connector configuration, offset, and status information.
The group ID and names of these topics must also be unique to the Kafka Connect cluster.</p>
</div>
<div class="paragraph">
<p>Workers are assigned one or more connector instances and tasks.
The distributed approach to deploying Kafka Connect is fault tolerant and scalable.
If a worker pod fails, the tasks it was running are reassigned to active workers.
You can add to a group of worker pods through configuration of the <code>replicas</code> property in the <code>KafkaConnect</code> resource.</p>
</div>
</div>
<div class="sect3">
<h4 id="transforms"><a class="link" href="#transforms">5.1.4. Transforms</a></h4>
<div class="paragraph">
<p>Kafka Connect translates and transforms external data.
Single-message transforms change messages into a format suitable for the target destination.
For example, a transform might insert or rename a field. Transforms can also filter and route data.
Plugins contain the implementation required for workers to perform one or more transformations.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Source connectors apply transforms before converting data into a format supported by Kafka.</p>
</li>
<li>
<p>Sink connectors apply transforms after converting data into a format suitable for an external data system.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>A transform comprises a set of Java class files packaged in a JAR file for inclusion in a connector plugin.
Kafka Connect provides a set of standard transforms, but you can also create your own.</p>
</div>
</div>
<div class="sect3">
<h4 id="converters"><a class="link" href="#converters">5.1.5. Converters</a></h4>
<div class="paragraph">
<p>When a worker receives data, it converts the data into an appropriate format using a converter.
You specify converters for workers in the worker <code>config</code> in the <code>KafkaConnect</code> resource.</p>
</div>
<div class="paragraph">
<p>Kafka Connect can convert data to and from formats supported by Kafka, such as JSON or Avro.
It also supports schemas for structuring data.
If you are not converting data into a structured format, you don’t need to enable schemas.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
You can also specify converters for specific connectors to override the general Kafka Connect worker configuration that applies to all workers.
</td>
</tr>
</table>
</div>
<div class="ulist _additional-resources">
<div class="title">Additional resources</div>
<ul>
<li>
<p><a href="http://kafka.apache.org" target="_blank" rel="noopener">Apache Kafka documentation</a></p>
</li>
<li>
<p><a href="./configuring.html#property-kafka-connect-config-reference" target="_blank" rel="noopener">Kafka Connect configuration of workers</a></p>
</li>
<li>
<p><a href="./deploying.html#proc-mirrormaker-replication-str" target="_blank" rel="noopener">Synchronizing data between Kafka clusters using MirrorMaker 2</a></p>
</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="con-overview-mm2-str"><a class="link" href="#con-overview-mm2-str">6. About MirrorMaker 2</a></h2>
<div class="sectionbody">
<div class="paragraph _abstract">
<p>Kafka MirrorMaker 2 replicates data (topics, consumer groups, and offsets) between Kafka clusters.
Through configuration, you define source and target cluster connections.
You can use MirrorMaker 2 in <em>active/passive</em> or <em>active/active</em> cluster configurations.</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">active/active cluster configuration</dt>
<dd>
<p>An active/active configuration has two active clusters replicating data bidirectionally. Applications can use either cluster. Each cluster can provide the same data. In this way,  you can make the same data available in different geographical locations. As consumer groups are active in both clusters, consumer offsets for replicated topics are not synchronized back to the source cluster.</p>
</dd>
<dt class="hdlist1">active/passive cluster configuration</dt>
<dd>
<p>An active/passive configuration has an active cluster replicating data to a passive cluster. The passive cluster remains on standby. You might use the passive cluster for data recovery in the event of system failure.</p>
</dd>
</dl>
</div>
<div class="paragraph">
<p>The expectation is that producers and consumers connect to active clusters only.
A MirrorMaker 2 cluster is required at each target destination.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
In most deployments, write operations should be made only to source topics.
Write operations on target topics are not prevented, but they can cause conflicts and should be avoided.
</td>
</tr>
</table>
</div>
<div class="sect2">
<h3 id="bidirectional_replication_activeactive"><a class="link" href="#bidirectional_replication_activeactive">6.1. Bidirectional replication (active/active)</a></h3>
<div class="paragraph">
<p>The MirrorMaker 2 architecture supports bidirectional replication in an <em>active/active</em> cluster configuration.</p>
</div>
<div class="paragraph">
<p>Each cluster replicates the other&#8217;s data using the concept of <em>source</em> and <em>remote</em> topics.
In this context, <em>remote</em> refers to the target cluster receiving replicated data from the perspective of the source MirrorMaker instance.
Each cluster simultaneously acts as both source and remote, depending on the direction of the replication flow.</p>
</div>
<div class="paragraph">
<p>As the same topics are stored in each cluster, remote topics are automatically renamed by MirrorMaker 2 to indicate their origin.
The name of the source cluster is prepended to the topic name.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="images/mirrormaker-renaming.png" alt="MirrorMaker 2 bidirectional architecture">
</div>
<div class="title">Figure 1. Topic renaming</div>
</div>
<div class="paragraph">
<p>This renaming prevents topics from being replicated back to their originating cluster.</p>
</div>
<div class="paragraph">
<p>This replication structure also supports data aggregation in active/active setups:
Consumers can subscribe to both source and remote topics within the same cluster, eliminating the need for a separate aggregation cluster.</p>
</div>
</div>
<div class="sect2">
<h3 id="unidirectional_replication_activepassive"><a class="link" href="#unidirectional_replication_activepassive">6.2. Unidirectional replication (active/passive)</a></h3>
<div class="paragraph">
<p>The MirrorMaker 2 architecture supports unidirectional replication in an <em>active/passive</em> cluster configuration.</p>
</div>
<div class="paragraph">
<p>You can use an <em>active/passive</em> cluster configuration to make backups or migrate data to another cluster.
In this situation, you might not want automatic renaming of remote topics.</p>
</div>
<div class="paragraph">
<p>You can override automatic renaming by adding <code>IdentityReplicationPolicy</code> to the source connector configuration.
With this configuration applied, topics retain their original names.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="overview-components-kafka-bridge_str"><a class="link" href="#overview-components-kafka-bridge_str">7. Kafka Bridge interface</a></h2>
<div class="sectionbody">
<div class="paragraph _abstract">
<p>The Kafka Bridge provides a RESTful interface that allows HTTP-based clients to interact with a Kafka cluster. 
It offers the advantages of a HTTP API connection to Strimzi for clients to produce and consume messages without the requirement to use the native Kafka protocol.</p>
</div>
<div class="paragraph">
<p>The API has two main resources — <code>consumers</code> and <code>topics</code> — that are exposed and made accessible through endpoints to interact with consumers and producers in your Kafka cluster. The resources relate only to the Kafka Bridge, not the consumers and producers connected directly to Kafka.</p>
</div>
<div class="sect2">
<h3 id="http_requests"><a class="link" href="#http_requests">7.1. HTTP requests</a></h3>
<div class="paragraph">
<p>The Kafka Bridge supports HTTP requests to a Kafka cluster, with methods to perform operations such as the following:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Send messages to a topic.</p>
</li>
<li>
<p>Retrieve messages from topics.</p>
</li>
<li>
<p>Retrieve a list of partitions for a topic.</p>
</li>
<li>
<p>Create and delete consumers.</p>
</li>
<li>
<p>Subscribe consumers to topics, so that they start receiving messages from those topics.</p>
</li>
<li>
<p>Retrieve a list of topics that a consumer is subscribed to.</p>
</li>
<li>
<p>Unsubscribe consumers from topics.</p>
</li>
<li>
<p>Assign partitions to consumers.</p>
</li>
<li>
<p>Commit a list of consumer offsets.</p>
</li>
<li>
<p>Seek on a partition, so that a consumer starts receiving messages from the first or last offset position, or a given offset position.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The methods provide JSON responses and HTTP response code error handling.
Messages can be sent in JSON or binary formats.</p>
</div>
<div class="ulist _additional-resources">
<div class="title">Additional resources</div>
<ul>
<li>
<p>To view the API documentation, including example requests and responses, see <a href="https://strimzi.io/docs/bridge/latest/" target="_blank" rel="noopener">Using the Kafka Bridge</a>.</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="con-overview-components-kafka-bridge-clients_str"><a class="link" href="#con-overview-components-kafka-bridge-clients_str">7.2. Supported clients for the Kafka Bridge</a></h3>
<div class="paragraph _abstract">
<p>You can use the Kafka Bridge to integrate both <em>internal</em> and <em>external</em> HTTP client applications with your Kafka cluster.</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Internal clients</dt>
<dd>
<p>Internal clients are container-based HTTP clients running in <em>the same</em> Kubernetes cluster as the Kafka Bridge itself.
Internal clients can access the Kafka Bridge on the host and port defined in the <code>KafkaBridge</code> custom resource.</p>
</dd>
<dt class="hdlist1">External clients</dt>
<dd>
<p>External clients are HTTP clients running <em>outside</em> the Kubernetes cluster in which the Kafka Bridge is deployed and running.
External clients can access the Kafka Bridge through an OpenShift Route, a loadbalancer service, or using an Ingress.</p>
</dd>
</dl>
</div>
<div class="paragraph">
<div class="title">HTTP internal and external client integration</div>
<p><span class="image"><img src="images/kafka-bridge.png" alt="Internal and external HTTP producers and consumers exchange data with the Kafka brokers through the Kafka Bridge"></span></p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="configuration-points_str"><a class="link" href="#configuration-points_str">8. Configuring Kafka</a></h2>
<div class="sectionbody">
<div class="paragraph _abstract">
<p>Strimzi enables the deployment of Kafka components on Kubernetes through highly configurable custom resources.
These resources leverage Custom Resource Definitions (CRDs) to extend Kubernetes capabilities, offering a flexible way to manage Kafka components.
After a new custom resource type is added to your cluster by installing a CRD, you can create instances of the resource based on its specification.
Many additional configuration options can be specified in a custom resource, some common and some specific to a particular component.</p>
</div>
<div class="paragraph">
<p>Use custom resources to configure the following components:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Kafka clusters</p>
</li>
<li>
<p>Node pools for Kafka clusters</p>
</li>
<li>
<p>MirrorMaker source and target clusters</p>
</li>
<li>
<p>Kafka Connect</p>
</li>
<li>
<p>Kafka Bridge</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Strimzi provides <a href="./deploying.html#config-examples-str" target="_blank" rel="noopener">example configuration files</a>, which can serve as a starting point when building your own Kafka component configuration for deployment.</p>
</div>
<div class="paragraph">
<p>For detailed configuration instructions and examples, see the guide for <a href="./deploying.html" target="_blank" rel="noopener">Deploying and Managing Strimzi</a>.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="security-overview_str"><a class="link" href="#security-overview_str">9. Securing Kafka</a></h2>
<div class="sectionbody">
<div class="paragraph _abstract">
<p>A secure deployment of Strimzi might encompass one or more of the following security measures:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Encryption</dt>
<dd>
<p>Strimzi supports Transport Layer Security (TLS), a protocol for encrypted communication.</p>
<div class="ulist">
<ul>
<li>
<p>Communication is always encrypted between Strimzi components.</p>
</li>
<li>
<p>To set up TLS-encrypted communication between Kafka and clients, you configure listeners in the <code>Kafka</code> custom resource.</p>
</li>
</ul>
</div>
</dd>
<dt class="hdlist1">Authentication</dt>
<dd>
<p>Kafka listeners use authentication to ensure a secure client connection to the Kafka cluster.
Clients can also be configured for mutual authentication.
Security credentials are created and managed by the Cluster and User Operator.</p>
<div class="paragraph">
<p>Supported authentication mechanisms:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>mTLS authentication (on listeners with TLS-enabled encryption)</p>
</li>
<li>
<p>SASL SCRAM-SHA-512</p>
</li>
<li>
<p>OAuth 2.0 token based authentication</p>
</li>
<li>
<p>Custom authentication (supported by Kafka)</p>
</li>
</ul>
</div>
</dd>
<dt class="hdlist1">Authorization</dt>
<dd>
<p>Authorization controls the operations that are permitted on Kafka brokers by specific clients or users.</p>
<div class="paragraph">
<p>Supported authorization mechanisms:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Simple authorization using ACL rules</p>
</li>
<li>
<p>OAuth 2.0 authorization (if you are using OAuth 2.0 token-based authentication)</p>
</li>
<li>
<p>Open Policy Agent (OPA) authorization (deprecated)</p>
</li>
<li>
<p>Custom authorization (supported by Kafka)</p>
</li>
</ul>
</div>
</dd>
<dt class="hdlist1">Federal Information Processing Standards (FIPS)</dt>
<dd>
<p>Strimzi is designed to run on FIPS-enabled Kubernetes clusters to ensure data security and system interoperability.
For more information about the NIST validation program and validated modules, see <a href="https://csrc.nist.gov/Projects/cryptographic-module-validation-program/validated-modules" target="_blank" rel="noopener">Cryptographic Module Validation Program</a> on the NIST website.</p>
</dd>
</dl>
</div>
</div>
</div>
<div class="sect1">
<h2 id="metrics-overview_str"><a class="link" href="#metrics-overview_str">10. Monitoring Kafka</a></h2>
<div class="sectionbody">
<div class="paragraph _abstract">
<p>Monitoring data allows you to monitor the performance and health of Strimzi.
You can configure your deployment to capture metrics data for analysis and notifications.</p>
</div>
<div class="paragraph">
<p>Metrics data is useful when investigating issues with connectivity and data delivery.
For example, metrics data can identify under-replicated partitions or the rate at which messages are consumed.
Alerting rules can provide time-critical notifications on such metrics through a specified communications channel.
Monitoring visualizations present real-time metrics data to help determine when and how to update the configuration of your deployment.
Example metrics configuration files are provided with Strimzi.</p>
</div>
<div class="paragraph">
<p>You can employ the following tools for metrics and monitoring:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Prometheus</dt>
<dd>
<p><a href="https://github.com/prometheus" target="_blank" rel="noopener">Prometheus</a> pulls metrics from Kafka, Kafka Connect, and other clusters.
The Prometheus <strong>Alertmanager</strong> plugin handles alerts and routes them to a notification service.</p>
</dd>
<dt class="hdlist1">Kafka Exporter</dt>
<dd>
<p><a href="https://github.com/danielqsj/kafka_exporter" target="_blank" rel="noopener">Kafka Exporter</a> adds additional Prometheus metrics.</p>
</dd>
<dt class="hdlist1">Grafana</dt>
<dd>
<p><a href="https://grafana.com/" target="_blank" rel="noopener">Grafana Labs</a> provides dashboard visualizations of Prometheus metrics.</p>
</dd>
<dt class="hdlist1">OpenTelemetry</dt>
<dd>
<p><a href="https://opentelemetry.io" target="_blank" rel="noopener">OpenTelemetry</a> complements the gathering of metrics data by providing a facility for end-to-end tracking of messages through Strimzi.</p>
</dd>
<dt class="hdlist1">Cruise Control</dt>
<dd>
<p><a href="https://github.com/linkedin/cruise-control" target="_blank" rel="noopener">Cruise Control</a> monitors data distribution and performs data rebalances, based on workload data, across a Kafka cluster.</p>
</dd>
</dl>
</div>
</div>
</div>